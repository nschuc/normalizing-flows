{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Normalizing Flows - VAE",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nschuc/normalizing-flows/blob/master/Normalizing_Flows_VAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "8OsY4hSNbfri",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Variational Inference with Normalizing Flows\n",
        "\n",
        "## Types of Generative Models\n",
        "\n",
        "\n",
        "1.   **Generative Aversarial Networks**:  Generator and Discriminator, discriminator learns to distinguish the real data from the fake samples that are produced by the generator model. \n",
        "2.   ** Variational Autoencoders**: VAE inexplicitly optimizes the log-likelyhood of the data by maximizing the evidence lower bound (ELBO)\n",
        "3. ** Flow-based** generative models: are constructed by a sequence of invertible transformations. Unlike GANs and VAEs the model explicitly learns the true data distribution $p(\\mathbf x)$ and the loss function is simply the negative log-likelyhood.\n",
        "\n",
        "\n",
        "Stolen from From [Lilian Weng's](https://lilianweng.github.io/lil-log/2018/10/13/flow-based-deep-generative-models.html#jacobian-matrix-and-determinant) blog:\n",
        "\n",
        "\n",
        "![alt text](https://lilianweng.github.io/lil-log/assets/images/three-generative-models.png)"
      ]
    },
    {
      "metadata": {
        "id": "FmtkFxoMbht-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kGjth91rT2ad",
        "colab_type": "code",
        "outputId": "87f4af2b-7213-4733-dfa4-4dfca9b4d805",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "! pip3 install torch torchvision\n",
        "! pip3 install pillow"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (0.4.1)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.2.1)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (5.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.14.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (5.3.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "R75oNahj5Z3S",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.utils.data\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import save_image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vxiJQitlLxBR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##  Normalizing Flows\n",
        "\n",
        "\n",
        "The basic rule for transformation of densities consideres an invertible, smooth mapping $f: \\mathbb{R}^D \\rightarrow  \\mathbb{R}^D$ with an inverse $f^{-1}=g$, such that  $ g \\circ f (\\textbf{z}) = \\textbf{z}$.  If we use this mapping to transform a random variable $\\mathbf{z}$ with distribution $q(\\mathbf{z})$, then the resulting random variable $\\mathbf{z}^\\prime = f(\\mathbf{z}$) has a distribution:\n",
        "\n",
        "$$\n",
        "q(\\mathbf{z}^{\\prime}) = q(\\mathbf{z}) \n",
        "  \\left| \n",
        "    \\det \\frac{\\partial f^{-1}}{\\partial \\mathbf{z}^\\prime}\n",
        "  \\right| = \n",
        "   q(\\mathbf{z}) \n",
        "  \\left| \n",
        "    \\det \\frac{\\partial f}{\\partial \\mathbf{z}}\n",
        "  \\right|^{-1},\n",
        "$$\n",
        "where the last equality can be obtained by applying [the inverse function theorem](https://en.wikipedia.org/wiki/Inverse_function_theorem) and taking advantage of the property of Jacobians of intertible functions.\n",
        "\n",
        "The density $q_K(\\mathbf z)$ obtained by successively transforming a random variable $\\mathbf z_0$ with distribution\n",
        "$q_0$ through a chain of $K$ transformations $f_k$ is:\n",
        "\n",
        "\\begin{align}\n",
        "  \\mathbf z_K &= f_K \\circ \\ldots \\circ f_1( \\mathbf z_0), \\\\\\\n",
        "  \\ln q_K (\\mathbf z_K) &= \\ln q_0(\\mathbf z_0) - \\sum_{k=1}^{K} \\ln \\det \\frac{\\partial f_k}{\\partial \\mathbf{z}_{k-1}}.\n",
        "\\end{align}\n",
        "\n",
        "The formalism of normalizing flows now gives us a systematic\n",
        "way of specifying the approximate posterior distributions\n",
        "$q(\\mathbf z| \\mathbf x)$ required for variational inference. With an\n",
        "appropriate choice of transformations $f_K$, we can initially\n",
        "use simple factorized distributions such as an independent\n",
        "Gaussian, and apply normalizing flows of different lengths\n",
        "to obtain increasingly complex and multi-modal distributions.\n",
        "\n",
        "\n",
        "\n",
        "From [Lilian Weng's  blog](https://lilianweng.github.io/lil-log/2018/10/13/flow-based-deep-generative-models.html#jacobian-matrix-and-determinant) (she uses $p_i$ instead of $q_i$):\n",
        "![alt text](https://lilianweng.github.io/lil-log/assets/images/normalizing-flow.png)\n",
        "\n",
        "###  Remark:\n",
        "If  $\\mathbf{p}$ is a point in $\\mathbb{R}^D$ and $f$ is differentiable at $\\mathbf{p}$, then its derivative is given by $J_f(\\mathbf{p})$. In this case, the linear map described by $J_f(\\mathbf{p})$ is the best linear approximation of $f$ near the point $\\mathbf{p}$, in the sense that\n",
        "\n",
        "$$\n",
        "\\mathbf f(\\mathbf x) = \\mathbf f(\\mathbf p) + \\mathbf J_{\\mathbf f}(\\mathbf p)(\\mathbf x - \\mathbf p) + o(\\|\\mathbf x - \\mathbf p\\|),\n",
        "$$\n",
        "\n",
        "where $\\mathbf x$ is close to $\\mathbf p$ and where $o$ is the little o-notation.\n",
        "\n",
        "Since, we can percieve the Jacobian of $f: \\mathbb{R}^D \\rightarrow  \\mathbb{R}^D$ as locally linear map, we can describe the space distortions using the determinant: geometrically the absolute value of the Jacobian determinant gives the magnification/scalling factor when we transform an area or volume. It intuitevely make sense, that if function changes the volume by $a$ it's inverse should change the volme by $\\frac{1}{a}$. "
      ]
    },
    {
      "metadata": {
        "id": "0uJ_n0H0_uqB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Abstract Flow"
      ]
    },
    {
      "metadata": {
        "id": "P4eU3GF1NtVG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from abc import ABC, abstractmethod\n",
        "from typing import List, Tuple\n",
        "from torch import Tensor\n",
        "\n",
        "class Flow(ABC):\n",
        "    @abstractmethod\n",
        "    def forward(self, z, parameters: Tuple[Tensor]) -> Tensor:\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def log_det_jacobian(self, z, parameters: Tuple[Tensor]):\n",
        "        pass\n",
        "      \n",
        "    @abstractmethod\n",
        "    def unpack(self, parameters: Tensor) -> Tuple[Tensor, ...]:\n",
        "        '''\n",
        "        Method used to unpack the hidden layer to parameters of the flow\n",
        "        \n",
        "        From section 4.2:\n",
        "        For amortized variational inference, we construct an inference model\n",
        "        using a deep neural network to build a mapping from the observations x\n",
        "        to the parameters of the initial density q0 = N(µ, σ) (µ∈R^D and σ∈R^D)\n",
        "        as well as the parameters of the flow λ.\n",
        "        '''\n",
        "        pass\n",
        "    \n",
        "    @property\n",
        "    @abstractmethod\n",
        "    def dim(self) -> int:\n",
        "        pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5QhpGnFeN3dm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Planar Flow"
      ]
    },
    {
      "metadata": {
        "id": "ZodQxYmbNyb4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def safe_log(z):\n",
        "    return torch.log(z + 1e-7)\n",
        "  \n",
        "def tanh(x):\n",
        "  return torch.tanh(x)\n",
        "\n",
        "\n",
        "def tanh_prime(x):\n",
        "  return 1 - torch.tanh(x)**2\n",
        "\n",
        "\n",
        "class PlanarFlow(nn.Module, Flow):\n",
        "  def __init__(self, dim = 2, h = tanh, h_prime= tanh_prime):\n",
        "      super().__init__()\n",
        "      '''\n",
        "      f(z) = z + u h(w^T @ z + b)\n",
        "      \n",
        "      \n",
        "      The flow defined by the transformation above modifies the\n",
        "      initial density q_0 by applying a series of contractions and\n",
        "      expansions in the direction perpendicular to the hyperplane\n",
        "      w^T z+b = 0, hence we refer to these maps as planar flows.\n",
        "      '''\n",
        "\n",
        "\n",
        "      # h(·) is a smooth element-wise non-linearity\n",
        "      self.h = h\n",
        "      self.h_prime = h_prime\n",
        "      self.d = dim\n",
        "       \n",
        "\n",
        "  def forward(self, z, parameters: Tuple[Tensor]) -> Tensor:\n",
        "      '''\n",
        "      f(z) = z + u h(w^T @ z + b)\n",
        "      '''\n",
        "      u, w, b = parameters\n",
        "      \n",
        "      z = z + self.h(F.linear(z, w, b)) @ u\n",
        "      return z\n",
        "  \n",
        "  def log_det_jacobian(self, z, parameters: Tuple[Tensor]):\n",
        "      '''\n",
        "      ψ(z) = h'(w^T @ z + b)w\n",
        "      |det @f/@z | = |1 + u^T ψ(z)|\n",
        "      \n",
        "      '''\n",
        "      u, w, b = parameters\n",
        "      \n",
        "      \n",
        "      psi = self.h_prime(F.linear(z, w, b)) @ w\n",
        "      det_jacobian = torch.abs(1 + F.linear(psi, u))\n",
        "      return safe_log(det_jacobian)\n",
        "    \n",
        "  def unpack(self, parameters: Tensor) -> Tuple[Tensor, ...]:\n",
        "      ''' unpacks the free parameters\n",
        "      λ = {w ∈ R^D, u ∈ R^D, b ∈ R} are free parameters\n",
        "      '''\n",
        "      w, u = parameters[:, :-1].chunk(2, dim=1)\n",
        "      b = parameters[:, -1].view(-1, 1)\n",
        "      \n",
        "      return (w, u, b)\n",
        "\n",
        "  @property\n",
        "  def dim(self):\n",
        "      return 2 * self.d + 1\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5O-SqUAON2wR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Radial Flow"
      ]
    },
    {
      "metadata": {
        "id": "tKorcChlNkKr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class RadialFlow(nn.Module, Flow):\n",
        "    def __init__(self, dim: int, h = tanh, h_prime= tanh_prime):\n",
        "      super().__init__()\n",
        "      '''\n",
        "      f(z) = z + βh(α, r)(z − z0),\n",
        "      \n",
        "      \n",
        "       It applies radial contractions and expansions\n",
        "       around the reference point and are thus referred to as\n",
        "       radial flows\n",
        "      '''\n",
        "      \n",
        "      # λ = {z_0 ∈ R^D, α ∈ R+, β ∈ R}\n",
        "\n",
        "\n",
        "      # h(·) is a smooth element-wise non-linearity\n",
        "      self.h = h\n",
        "      self.h_prime = h_prime\n",
        "      \n",
        "    \n",
        "    \n",
        "    def forward(self, z, parameters: Tensor) -> Tensor:\n",
        "        pass\n",
        "\n",
        "    def log_det_jacobian(self, z, parameters: Tensor):\n",
        "        pass\n",
        "      \n",
        "    def unpack(self, parameters: Tensor) -> Tuple[Tensor, ...]:\n",
        "        pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TQyEB9dMN83u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Normalizing Flows"
      ]
    },
    {
      "metadata": {
        "id": "fgVIWd-wNnZw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class NormalizingFlow(nn.Module):\n",
        "    def __init__(self, K: int, flow_class, *args, **kwargs):\n",
        "        super().__init__()\n",
        "        self.flows = nn.Sequential(*(\n",
        "            flow_class(*args, **kwargs) for _ in range(K)\n",
        "        ))\n",
        "      \n",
        "    def forward(self, z, lambdas: List[Tensor]):\n",
        "      log_abs_det_jacobians = []\n",
        "\n",
        "      for flow, parameters in zip(self.flows, lambdas):\n",
        "          log_abs_det_jacobians.append(flow.log_det_jacobian(z, parameters))\n",
        "          z = flow(z, parameters)\n",
        "          \n",
        "      return z, sum(log_abs_det_jacobians)\n",
        "    \n",
        "    def unpack(self, params: Tensor) -> List[Tensor]:\n",
        "      flow_params = []\n",
        "      start, end = 0, 0\n",
        "      \n",
        "      for flow in self.flows:\n",
        "        start, end = end, end + flow.dim\n",
        "        flow_params.append(flow.unpack(params[:, start:end]))\n",
        "\n",
        "      return flow_params\n",
        "    \n",
        "    @property\n",
        "    def dims(self):\n",
        "      return sum(flow.dim for flow in self.flows)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EJV3bqpfG4wD",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def h(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "def h_prime(x):\n",
        "    return 1 - np.tanh(x) ** 2\n",
        "\n",
        "def f(z, w, u, b):\n",
        "    return z + np.dot(h(np.dot(z, w) + b).reshape(-1,1), u.reshape(1,-1))\n",
        "  \n",
        "  \n",
        "def plot_flow():\n",
        "  plt.figure(figsize=[10, 14])\n",
        "\n",
        "  id_figure = 1\n",
        "  for i in np.arange(5):\n",
        "      for j in np.arange(5):\n",
        "          #represent w and u in polar coordinate system\n",
        "          theta_w = 0\n",
        "          rho_w = 5\n",
        "          theta_u = np.pi / 8 * i\n",
        "          rho_u = j / 4.0\n",
        "          \n",
        "          w = np.array([np.cos(theta_w), np.sin(theta_w)]) * rho_w\n",
        "          u = np.array([np.cos(theta_u), np.sin(theta_u)]) * rho_u\n",
        "          b = 0\n",
        "\n",
        "          grid_use = np.meshgrid(np.arange(-1,1,0.001), np.arange(-1,1,0.001))\n",
        "          z = np.concatenate([grid_use[0].reshape(-1,1), grid_use[1].reshape(-1,1)], axis=1)\n",
        "          z = np.random.normal(size=(int(1e6),2))\n",
        "          z_new = f(z, w, u, b)\n",
        "\n",
        "          heatmap, xedges, yedges = np.histogram2d(\n",
        "              z_new[:,0], z_new[:,1], bins=50, range=[[-3,3],[-3,3]])\n",
        "\n",
        "          extent = [xedges[0], xedges[-1], yedges[0], yedges[-1]]\n",
        "\n",
        "          plt.subplot(5,5,id_figure)\n",
        "          plt.imshow(heatmap, extent=extent, cmap='viridis')\n",
        "          plt.title(\"u=(%.1f,%.1f)\"%(u[0],u[1]) + \"\\n\" +\n",
        "                    \"w=(%d,%d)\"%(w[0],w[1]) + \", \" + \"b=%d\"%b)\n",
        "          id_figure += 1\n",
        "\n",
        "          plt.xlim([-3,3])\n",
        "          plt.ylim([-3,3])\n",
        "        \n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bE9jgLd9jR9V",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The effect of planar and radial flows on the Gaussian and uniform distributions. The figure comes from the original paper.\n",
        "![alt text](http://akosiorek.github.io/resources/simple_flows.png)"
      ]
    },
    {
      "metadata": {
        "id": "C1U1coegJJHL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Distributions"
      ]
    },
    {
      "metadata": {
        "id": "x3C0y_fzJGhu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import math\n",
        "import random\n",
        "\n",
        "from numbers import Number\n",
        "from itertools import accumulate\n",
        "from bisect import bisect_right\n",
        "\n",
        "\n",
        "  \n",
        "def p_1(z, shift = 2, scale_1 = 0.4 , scale_2 = 0.6):\n",
        "    '''\n",
        "    Unnormalized fancy 2D density number 1\n",
        "    '''\n",
        "\n",
        "    z1, z2 = torch.chunk(z, chunks=2, dim=1)\n",
        "    norm = torch.sqrt(z1 ** 2 + z2 ** 2)\n",
        "\n",
        "    exp1 = torch.exp(-0.5 * ((z1 - shift) / scale_2) ** 2)\n",
        "    exp2 = torch.exp(-0.5 * ((z1 + shift) / scale_2) ** 2)\n",
        "    u = 0.5 * ((norm - shift) / scale_1) ** 2 - safe_log(exp1 + exp2)\n",
        "\n",
        "    return torch.exp(-u)\n",
        "\n",
        "\n",
        "def sum_probs(point_1, point_2):\n",
        "  z_1, x_1, y_1 = point_1\n",
        "  z_2, x_2, y_2 = point_2\n",
        "  return z_1 + z_2, x_2, y_2\n",
        "\n",
        "\n",
        "def find_le(a, x):\n",
        "    'Find rightmost value less than or equal to x'\n",
        "    i = bisect_right(a, (x, ))\n",
        "    if i:\n",
        "        return a[i-1]\n",
        "    raise ValueError\n",
        "\n",
        "def sample(points):\n",
        "  p, x, y = find_le(points, random.random())\n",
        "  return (x, y)\n",
        "\n",
        "\n",
        "class EmpiricalSampler:\n",
        "  def __init__(self,  \n",
        "               density,\n",
        "               n_points: int = 600, \n",
        "               limits: Tuple[float] = (-4, 4)):\n",
        "    '''\n",
        "    Wrapper class to sample from a close form bivariate-pdf\n",
        "    '''\n",
        "    self.density = density\n",
        "\n",
        "    x = np.linspace(*limits, n_points)\n",
        "    y = np.linspace(*limits, n_points)\n",
        "    x, y = np.meshgrid(x, y)\n",
        "    z = density(Tensor(np.c_[x, y])).data.numpy().reshape((n_points, n_points))\n",
        "    z = z / np.sum(z)\n",
        "    \n",
        "    points = zip(z.ravel(), x.ravel(), y.ravel())\n",
        "    points = list(accumulate(points, sum_probs))\n",
        "    self.points = points\n",
        "    \n",
        "  def sample(self, n: int):\n",
        "    return np.array([sample(self.points) for _ in range(n)])\n",
        "\n",
        "\n",
        "class Gaussian:\n",
        "    def __init__(self, dim: int = 2):\n",
        "      self.d = 2\n",
        "      \n",
        "    def unpack(self, parameters: Tensor) -> Tuple[Tensor, ...]:\n",
        "      ''' takes hidden state and returns mu and sigma'''\n",
        "      mu, log_var = parameters.chunk(2, dim=1)\n",
        "      std = torch.exp(0.5*log_var)\n",
        "      \n",
        "      return mu, std \n",
        "    \n",
        "    def sample_with_log_prob(self, n, parameters: Tensor):\n",
        "      mu, std = parameters\n",
        "      \n",
        "      xs = mu + std * Gaussian.sample(n, self.d)\n",
        "\n",
        "      return xs, self.log_prob(xs, mu, std)\n",
        "    \n",
        "    @classmethod\n",
        "    def sample(cls, n: int, d: int, mean: float = 0, std:float = 1):\n",
        "        return torch.zeros(n, d).normal_(mean=mean, std=std)\n",
        "    \n",
        "    @classmethod\n",
        "    def log_prob(cls, value, mu, std):\n",
        "      '''\n",
        "      Log of density function of multivariate normal with diagonal \n",
        "      covariance matrix\n",
        "      '''\n",
        "      var = std ** 2\n",
        "      log_std = math.log(std) if isinstance(std, Number) else std.log()\n",
        "      return (-((value - mu) ** 2) / (2 * var) - log_std - math.log(math.sqrt(2 * math.pi) )).sum(1, True)\n",
        "\n",
        "    \n",
        "    @property\n",
        "    def dims(self):\n",
        "      ''' \n",
        "      params for mu and sigma\n",
        "      '''\n",
        "      return 2 * self.d"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WLmXEOcGBS3q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Flow-Based Free Energy Bound"
      ]
    },
    {
      "metadata": {
        "id": "3XO9pTUm5r1l",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class FreeEnergyBound(nn.Module):\n",
        "\n",
        "    def __init__(self, p_x):\n",
        "        super().__init__()\n",
        "        self.p_x = p_x\n",
        "        \n",
        "\n",
        "    def forward(self, log_q_0, z_k, log_jacobians, beta):\n",
        "        energy =  log_q_0 \\\n",
        "                - beta * safe_log(self.p_x.density(z_k)) \\\n",
        "                - log_jacobians\n",
        "        \n",
        "        return energy.mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vlAW9GM_GUnK",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title\n",
        "import os\n",
        "from torch.autograd import Variable\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "def scatter_points(points, directory, iteration, flow_length):\n",
        "\n",
        "    X_LIMS = (-4, 4)\n",
        "    Y_LIMS = (-4, 4)\n",
        "\n",
        "    fig = plt.figure(figsize=(7, 7))\n",
        "    ax = fig.add_subplot(111)\n",
        "    ax.scatter(points[:, 0], points[:, 1], alpha=0.7, s=25)\n",
        "    ax.set_xlim(*X_LIMS)\n",
        "    ax.set_ylim(*Y_LIMS)\n",
        "    ax.set_title(\n",
        "        \"Flow length: {}\\n Samples on iteration #{}\"\n",
        "        .format(flow_length, iteration)\n",
        "    )\n",
        "\n",
        "    fig.savefig(os.path.join(directory, \"flow_result_{}.png\".format(iteration)))\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def plot_density(distribution, directory):\n",
        "\n",
        "    X_LIMS = (-4, 4)\n",
        "    Y_LIMS = (-4, 4)\n",
        "\n",
        "    x1 = np.linspace(*X_LIMS, 300)\n",
        "    x2 = np.linspace(*Y_LIMS, 300)\n",
        "    x1, x2 = np.meshgrid(x1, x2)\n",
        "    shape = x1.shape\n",
        "    x1 = x1.ravel()\n",
        "    x2 = x2.ravel()\n",
        "\n",
        "    z = np.c_[x1, x2]\n",
        "    z = torch.FloatTensor(z)\n",
        "    z = Variable(z)\n",
        "\n",
        "    density_values = distribution.density(z).data.numpy().reshape(shape)\n",
        "\n",
        "    fig = plt.figure(figsize=(7, 7))\n",
        "    ax = fig.add_subplot(111)\n",
        "    ax.imshow(density_values, extent=(*X_LIMS, *Y_LIMS), cmap=\"summer\")\n",
        "    ax.set_title(\"True density\")\n",
        "\n",
        "    fig.savefig(os.path.join(directory, \"density.png\"))\n",
        "    plt.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "swCIzq5SMo6q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from itertools import chain\n",
        "\n",
        "\n",
        "def init_weights(m):\n",
        "  if type(m) == nn.Linear:\n",
        "      torch.nn.init.xavier_uniform_(m.weight)\n",
        "      m.bias.data.fill_(0.01)\n",
        "\n",
        "\n",
        "class Maxout(nn.Module):\n",
        "    def __init__(self, pool_size: int = 4):\n",
        "        super().__init__()\n",
        "        self._pool_size = pool_size\n",
        "\n",
        "    def forward(self, x):\n",
        "        assert x.shape[-1] % self._pool_size == 0, \\\n",
        "            f\"Wrong input last dim size ({x.shape[-1]}) for Maxout({self._pool_size})\"\n",
        "        \n",
        "        m, i = x.view(*x.shape[:-1], x.shape[-1] // self._pool_size, self._pool_size).max(-1)\n",
        "        \n",
        "        return m\n",
        "      \n",
        "\n",
        "class InferenceNetwork(nn.Module):\n",
        "    def __init__(self, distribution, flows, sizes: List[Tuple[int]], pool_size: int = 4):\n",
        "      '''\n",
        "      Inference model using a deep neural network to build a mapping\n",
        "      from the observations x to the parameters.\n",
        "      \n",
        "      '''\n",
        "      super().__init__()\n",
        "      \n",
        "   \n",
        "      self.flows = flows\n",
        "      self.distribution = distribution\n",
        "      \n",
        "      layers = list(chain.from_iterable([\n",
        "          (nn.Linear(d_in, d_out*pool_size), Maxout(pool_size=pool_size)) for d_in, d_out in sizes\n",
        "      ]))\n",
        "      \n",
        "      d_in = sizes[-1][-1]\n",
        "      d_out = (self.flows.dims + self.distribution.dims) * pool_size\n",
        "      \n",
        "      layers.append(nn.Linear(d_in, d_out))\n",
        "      layers.append(Maxout(pool_size))\n",
        "      \n",
        "      \n",
        "      self.net = nn.Sequential(*layers)\n",
        "      self.net.apply(init_weights)\n",
        "      \n",
        "    \n",
        "    def forward(self, x):\n",
        "      parameters = self.net(x)\n",
        "      # unpack initial distribution parameters\n",
        "      start, end = 0, self.distribution.dims\n",
        "      dist_params = self.distribution.unpack(parameters[:, start:end])\n",
        "      \n",
        "      # unpack flow parameters\n",
        "      flow_params = self.flows.unpack(parameters[:, start:])\n",
        "      \n",
        "      return dist_params, flow_params\n",
        "    \n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NLFdE4bLw8q4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training"
      ]
    },
    {
      "metadata": {
        "id": "9hN4WHBocAFy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "directory = '/content/results/'\n",
        "if not os.path.exists(directory):\n",
        "  os.makedirs(directory)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "WjVetqBRHrJx",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "K = 2\n",
        "\n",
        "fancy_dist = EmpiricalSampler(p_1)\n",
        "\n",
        "q_0 = Gaussian(2)\n",
        "flow = NormalizingFlow(K=K, flow_class=PlanarFlow, dim=2)\n",
        "\n",
        "net = InferenceNetwork(q_0, flow, [(2, 100)])\n",
        "annealed_bound = FreeEnergyBound(p_x = fancy_dist)\n",
        "\n",
        "optimizer = optim.RMSprop(net.parameters(), lr=1e-5, momentum=0.9)\n",
        "\n",
        "for iteration in range(1, 500000):\n",
        "    \n",
        "    # get samples from the true distribution\n",
        "    true_samples = Tensor(fancy_dist.sample(100))\n",
        "    \n",
        "    # use inference network to find the parameters of the\n",
        "\n",
        "    dist_params, flow_params = net(true_samples)\n",
        "    z_0, log_q_0 = q_0.sample_with_log_prob(100, dist_params)\n",
        "    z_k, log_jacobians = flow(z_0, flow_params)\n",
        "    loss = annealed_bound(log_q_0, z_k, log_jacobians, min(1, 0.01 + iteration/1000))\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    if iteration % 1000 == 0:\n",
        "      print(\"Loss on iteration {}: {}\".format(iteration , loss.data.item()))\n",
        "    \n",
        "    if iteration % 10000 == 0:\n",
        "        scatter_points(\n",
        "            z_k.data.numpy(),\n",
        "            directory='/content/results/',\n",
        "            iteration=iteration,\n",
        "            flow_length=K\n",
        "        )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tLr0ie6Z_igv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Vatiational Autoencoders"
      ]
    },
    {
      "metadata": {
        "id": "82AL27KjxMd_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Vanilla VAE"
      ]
    },
    {
      "metadata": {
        "id": "g3CO-FWqSUdH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class VAE(nn.Module):\n",
        "    '''\n",
        "    On Mnist\n",
        "    '''\n",
        "    def __init__(self, \n",
        "                 feature_size: int = 784,\n",
        "                 hidden_size: int = 400,\n",
        "                 code_size: int = 20):\n",
        "        super(VAE, self).__init__()\n",
        "        self.code_size = code_size\n",
        "        self.feature_size = feature_size\n",
        " \n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(feature_size, hidden_size),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(hidden_size, code_size * 2)\n",
        "        )\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(code_size, hidden_size),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(hidden_size, feature_size),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "    \n",
        "    def reparameterize(self, mu, log_var):\n",
        "        std = torch.exp(0.5*log_var)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + std*eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, self.feature_size)\n",
        "        \n",
        "        mu, log_var = self.encoder(x).chunk(2, dim=1)\n",
        "\n",
        "        z = self.reparameterize(mu, log_var)\n",
        "        \n",
        "        kl_div = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
        "        kl_div = kl_div / x.size(0)  # mean over batch\n",
        "        \n",
        "        return self.decoder(z), kl_div"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-S0lQXDsVhMK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def reconstruction_loss(recon_x, x):\n",
        "    # batch mean\n",
        "    return F.binary_cross_entropy(recon_x, x, reduction=\"sum\") / x.size(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YFYcafr0SZma",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Trainer:\n",
        "    def __init__(self, model, train_loader, test_loader,\n",
        "                 log_interval: int =10,\n",
        "                 batch_size: int =128):\n",
        "      self.model = model\n",
        "      self.train_loader = train_loader\n",
        "      self.test_loader = train_loader\n",
        "\n",
        "      self.log_interval = log_interval\n",
        "      self.batch_size = batch_size\n",
        "      \n",
        "      self.optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "    def train(self, epoch):\n",
        "        self.model.train()\n",
        "        train_loss = 0\n",
        "        for batch_idx, (data, _) in enumerate(self.train_loader):\n",
        "            data = data.to(device)\n",
        "            self.optimizer.zero_grad()\n",
        "            recon_batch, kl_div = self.model(data)\n",
        "            recon_loss = reconstruction_loss(recon_batch, data)\n",
        "            loss =  recon_loss + kl_div\n",
        "            loss.backward()\n",
        "            train_loss += loss.item()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            if batch_idx % self.log_interval == 0:\n",
        "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                    epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                    100. * batch_idx / len(train_loader),\n",
        "                    loss.item() / len(data)))\n",
        "                \n",
        "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tRecon: {:.6f}'.format(\n",
        "                    epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                    100. * batch_idx / len(train_loader),\n",
        "                    recon_loss.item() / len(data)))\n",
        "                \n",
        "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tKL: {:.6f}\\n'.format(\n",
        "                    epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                    100. * batch_idx / len(train_loader),\n",
        "                    kl_div.item() / len(data)))\n",
        "                \n",
        "\n",
        "        print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
        "              epoch, train_loss / len(train_loader.dataset)))\n",
        "\n",
        "\n",
        "    def test(self, epoch, name, fixed_sample = None):\n",
        "        self.model.eval()\n",
        "        test_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for i, (data, _) in enumerate(self.test_loader):\n",
        "                data = data.to(device)                \n",
        "                recon_batch, kl_div = self.model(data)\n",
        "                test_loss += (reconstruction_loss(recon_batch, data) + kl_div).item()\n",
        "                \n",
        "                if i == 0:\n",
        "                    n = min(data.size(0), 8)\n",
        "                    comparison = torch.cat([\n",
        "                        data[:n],\n",
        "                        recon_batch.view(self.batch_size, 1, 28, 28)[:n]\n",
        "                    ])\n",
        "                    \n",
        "                    save_image(comparison.cpu(),\n",
        "                             f'/content/results/reconstruction_{name}_{epoch}.png', nrow=n)\n",
        "              \n",
        "            if not fixed_sample is None:\n",
        "                fixed_sample = fixed_sample.to(device)\n",
        "                recon_sample, _ = self.model(fixed_sample)\n",
        "                recon_sample = recon_sample.view(8, 1, 28, 28)\n",
        "                \n",
        "                n = min(fixed_sample.size(0), 8)\n",
        "                comparison = torch.cat([\n",
        "                    fixed_sample[:n],\n",
        "                    recon_sample[:n]\n",
        "                ])\n",
        "                \n",
        "                save_image(comparison.cpu(),\n",
        "                           f'/content/results/fixed_reconstruction_{name}_{epoch}.png', nrow=n)   \n",
        "              \n",
        "\n",
        "        test_loss /= len(test_loader.dataset)\n",
        "        print('====> Test set loss: {:.4f}'.format(test_loss))\n",
        "        \n",
        "      \n",
        "    def run(self, num_epochs, name, fixed_sample = None):\n",
        "        for epoch in range(1, num_epochs + 1):\n",
        "          self.train(epoch)\n",
        "          self.test(epoch, name, fixed_sample)\n",
        "          with torch.no_grad():\n",
        "              sample = torch.randn(64, model.code_size).to(device)\n",
        "              sample = model.decoder(sample).cpu()\n",
        "              sample_name = f\"results/sample_{name}_{epoch}.png\"\n",
        "              save_image(sample.view(64, 1, 28, 28), sample_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BnWkSbKZSxT9",
        "colab_type": "code",
        "outputId": "11721c57-f4f9-490f-866d-3f6dc69e6458",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\")\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('data', train=True, download=True, transform=transforms.ToTensor()),\n",
        "    batch_size=128, shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('data', train=False, transform=transforms.ToTensor()),\n",
        "    batch_size=128, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Processing...\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "USdDkY_gS9eI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = VAE(code_size=40)\n",
        "model.to(device)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model = model, \n",
        "    train_loader = train_loader,\n",
        "    test_loader = test_loader,\n",
        "    log_interval = 100\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fdPoLDEtTDWA",
        "colab_type": "code",
        "outputId": "f452b98f-4b30-46e9-c5c1-342e7753084d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3845
        }
      },
      "cell_type": "code",
      "source": [
        "trainer.run(10, 'VAE')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1594: UserWarning: Using a target size (torch.Size([128, 1, 28, 28])) that is different to the input size (torch.Size([128, 784])) is deprecated. Please ensure they have the same size.\n",
            "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 4.260551\n",
            "Train Epoch: 1 [0/60000 (0%)]\tRecon: 4.258914\n",
            "Train Epoch: 1 [0/60000 (0%)]\tKL: 0.001637\n",
            "\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.510920\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tRecon: 1.441620\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tKL: 0.069300\n",
            "\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 1.251006\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tRecon: 1.132936\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tKL: 0.118070\n",
            "\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 1.176127\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tRecon: 1.034529\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tKL: 0.141598\n",
            "\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 1.039194\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tRecon: 0.877119\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tKL: 0.162075\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1594: UserWarning: Using a target size (torch.Size([96, 1, 28, 28])) that is different to the input size (torch.Size([96, 784])) is deprecated. Please ensure they have the same size.\n",
            "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "====> Epoch: 1 Average loss: 1.3236\n",
            "====> Test set loss: 6.3234\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 1.107982\n",
            "Train Epoch: 2 [0/60000 (0%)]\tRecon: 0.930139\n",
            "Train Epoch: 2 [0/60000 (0%)]\tKL: 0.177844\n",
            "\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 1.007832\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tRecon: 0.828177\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tKL: 0.179655\n",
            "\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.976398\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tRecon: 0.791094\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tKL: 0.185304\n",
            "\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.948154\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tRecon: 0.762266\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tKL: 0.185889\n",
            "\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.952687\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tRecon: 0.756284\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tKL: 0.196403\n",
            "\n",
            "====> Epoch: 2 Average loss: 0.9862\n",
            "====> Test set loss: 5.6337\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.922349\n",
            "Train Epoch: 3 [0/60000 (0%)]\tRecon: 0.740641\n",
            "Train Epoch: 3 [0/60000 (0%)]\tKL: 0.181709\n",
            "\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.938758\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tRecon: 0.744494\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tKL: 0.194264\n",
            "\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.938921\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tRecon: 0.745210\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tKL: 0.193712\n",
            "\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.914270\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tRecon: 0.717480\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tKL: 0.196790\n",
            "\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.898740\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tRecon: 0.701114\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tKL: 0.197626\n",
            "\n",
            "====> Epoch: 3 Average loss: 0.9148\n",
            "====> Test set loss: 5.3674\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.885070\n",
            "Train Epoch: 4 [0/60000 (0%)]\tRecon: 0.689027\n",
            "Train Epoch: 4 [0/60000 (0%)]\tKL: 0.196043\n",
            "\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.856391\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tRecon: 0.659111\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tKL: 0.197280\n",
            "\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.864966\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tRecon: 0.656940\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tKL: 0.208025\n",
            "\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.882357\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tRecon: 0.683134\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tKL: 0.199223\n",
            "\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.877373\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tRecon: 0.681058\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tKL: 0.196315\n",
            "\n",
            "====> Epoch: 4 Average loss: 0.8832\n",
            "====> Test set loss: 5.2217\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.870865\n",
            "Train Epoch: 5 [0/60000 (0%)]\tRecon: 0.671205\n",
            "Train Epoch: 5 [0/60000 (0%)]\tKL: 0.199660\n",
            "\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.861315\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tRecon: 0.658991\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tKL: 0.202324\n",
            "\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.867669\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tRecon: 0.661856\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tKL: 0.205813\n",
            "\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.876682\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tRecon: 0.669823\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tKL: 0.206859\n",
            "\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.863820\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tRecon: 0.659938\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tKL: 0.203882\n",
            "\n",
            "====> Epoch: 5 Average loss: 0.8645\n",
            "====> Test set loss: 5.1407\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.866413\n",
            "Train Epoch: 6 [0/60000 (0%)]\tRecon: 0.661814\n",
            "Train Epoch: 6 [0/60000 (0%)]\tKL: 0.204598\n",
            "\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.851403\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tRecon: 0.647492\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tKL: 0.203911\n",
            "\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.873897\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tRecon: 0.665599\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tKL: 0.208297\n",
            "\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.843521\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tRecon: 0.644161\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tKL: 0.199360\n",
            "\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.856668\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tRecon: 0.652945\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tKL: 0.203723\n",
            "\n",
            "====> Epoch: 6 Average loss: 0.8531\n",
            "====> Test set loss: 5.0840\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.845427\n",
            "Train Epoch: 7 [0/60000 (0%)]\tRecon: 0.641470\n",
            "Train Epoch: 7 [0/60000 (0%)]\tKL: 0.203957\n",
            "\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.842331\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tRecon: 0.641944\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tKL: 0.200387\n",
            "\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.860531\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tRecon: 0.658648\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tKL: 0.201883\n",
            "\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.834895\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tRecon: 0.626617\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tKL: 0.208278\n",
            "\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.890155\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tRecon: 0.679917\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tKL: 0.210237\n",
            "\n",
            "====> Epoch: 7 Average loss: 0.8453\n",
            "====> Test set loss: 5.0471\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.858472\n",
            "Train Epoch: 8 [0/60000 (0%)]\tRecon: 0.650979\n",
            "Train Epoch: 8 [0/60000 (0%)]\tKL: 0.207493\n",
            "\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.835034\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tRecon: 0.631208\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tKL: 0.203826\n",
            "\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.841522\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tRecon: 0.635119\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tKL: 0.206403\n",
            "\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.793328\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tRecon: 0.596024\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tKL: 0.197304\n",
            "\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.830624\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tRecon: 0.624213\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tKL: 0.206411\n",
            "\n",
            "====> Epoch: 8 Average loss: 0.8390\n",
            "====> Test set loss: 5.0161\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.842742\n",
            "Train Epoch: 9 [0/60000 (0%)]\tRecon: 0.628235\n",
            "Train Epoch: 9 [0/60000 (0%)]\tKL: 0.214507\n",
            "\n",
            "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.821546\n",
            "Train Epoch: 9 [12800/60000 (21%)]\tRecon: 0.614095\n",
            "Train Epoch: 9 [12800/60000 (21%)]\tKL: 0.207451\n",
            "\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.833850\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tRecon: 0.624694\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tKL: 0.209156\n",
            "\n",
            "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.827051\n",
            "Train Epoch: 9 [38400/60000 (64%)]\tRecon: 0.625552\n",
            "Train Epoch: 9 [38400/60000 (64%)]\tKL: 0.201499\n",
            "\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.832641\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tRecon: 0.629347\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tKL: 0.203294\n",
            "\n",
            "====> Epoch: 9 Average loss: 0.8345\n",
            "====> Test set loss: 4.9941\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.800817\n",
            "Train Epoch: 10 [0/60000 (0%)]\tRecon: 0.602075\n",
            "Train Epoch: 10 [0/60000 (0%)]\tKL: 0.198742\n",
            "\n",
            "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.843513\n",
            "Train Epoch: 10 [12800/60000 (21%)]\tRecon: 0.635339\n",
            "Train Epoch: 10 [12800/60000 (21%)]\tKL: 0.208174\n",
            "\n",
            "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.835286\n",
            "Train Epoch: 10 [25600/60000 (43%)]\tRecon: 0.632137\n",
            "Train Epoch: 10 [25600/60000 (43%)]\tKL: 0.203149\n",
            "\n",
            "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.813342\n",
            "Train Epoch: 10 [38400/60000 (64%)]\tRecon: 0.607464\n",
            "Train Epoch: 10 [38400/60000 (64%)]\tKL: 0.205877\n",
            "\n",
            "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.839418\n",
            "Train Epoch: 10 [51200/60000 (85%)]\tRecon: 0.633010\n",
            "Train Epoch: 10 [51200/60000 (85%)]\tKL: 0.206408\n",
            "\n",
            "====> Epoch: 10 Average loss: 0.8312\n",
            "====> Test set loss: 4.9694\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YeyDr4LiScZC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## VAE with Normalizing Flows"
      ]
    },
    {
      "metadata": {
        "id": "J2k4NFnx_mcF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class VAE_NF(nn.Module):\n",
        "    '''\n",
        "    On Mnist\n",
        "    '''\n",
        "    def __init__(self, \n",
        "                 flows: NormalizingFlow,\n",
        "                 feature_size: int = 784,\n",
        "                 hidden_size: int = 400,\n",
        "                 code_size: int = 20):\n",
        "        super(VAE_NF, self).__init__()\n",
        "        self.flow = flow\n",
        "        self.code_size = code_size\n",
        "        \n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(feature_size, hidden_size),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(hidden_size, code_size * 2 + self.flow.dims)\n",
        "        )\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(code_size, hidden_size),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(hidden_size, feature_size),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5*logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + std*eps\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Run inference network to get the posterior parameters\n",
        "        params = self.encoder(x.view(-1, 784))\n",
        "        \n",
        "        mu = params[:, :self.code_size]\n",
        "        log_var = params[:, self.code_size: self.code_size * 2]\n",
        "        flow_params = self.flow.unpack(params[:, self.code_size*2:])\n",
        "        \n",
        "        # Get samples from posterior\n",
        "        z = self.reparameterize(mu, log_var)\n",
        "        z_K, log_jacobians = self.flow(z, flow_params)\n",
        "        \n",
        "        # Push it through generative network\n",
        "        x_recon = self.decoder(z_K)\n",
        "        \n",
        "        # Calculate the loss\n",
        "        kl_div = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
        "        kl_div = kl_div / x.size(0) - log_jacobians.mean()\n",
        "        \n",
        "        return x_recon, kl_div\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-P6b3ZGOAqHU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "code_size = 40\n",
        "\n",
        "flow = NormalizingFlow(K=4, flow_class=PlanarFlow, dim=code_size)\n",
        "\n",
        "model = VAE_NF(flow, code_size=code_size)\n",
        "model.to(device)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model = model, \n",
        "    train_loader = train_loader,\n",
        "    test_loader = test_loader,\n",
        "    log_interval = 100\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h5KQp9GYo3-9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "fixed_sample, _ = next(iter(test_loader))\n",
        "fixed_sample = fixed_sample[:8]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "80x_hplOPet2",
        "colab_type": "code",
        "outputId": "7552b8c2-8428-4839-9e63-543c805dd4ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37505
        }
      },
      "cell_type": "code",
      "source": [
        "trainer.run(100, 'VAE_NF', fixed_sample)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1594: UserWarning: Using a target size (torch.Size([128, 1, 28, 28])) that is different to the input size (torch.Size([128, 784])) is deprecated. Please ensure they have the same size.\n",
            "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 6.022123\n",
            "Train Epoch: 1 [0/60000 (0%)]\tRecon: 6.038678\n",
            "Train Epoch: 1 [0/60000 (0%)]\tKL: -0.016555\n",
            "\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.741269\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tRecon: 1.683798\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tKL: 0.057471\n",
            "\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 1.836907\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tRecon: 1.709057\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tKL: 0.127850\n",
            "\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 1.679492\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tRecon: 1.645444\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tKL: 0.034048\n",
            "\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 1.695807\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tRecon: 1.667954\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tKL: 0.027853\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1594: UserWarning: Using a target size (torch.Size([96, 1, 28, 28])) that is different to the input size (torch.Size([96, 784])) is deprecated. Please ensure they have the same size.\n",
            "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "====> Epoch: 1 Average loss: 1.8708\n",
            "====> Test set loss: 11.8883\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 1.983974\n",
            "Train Epoch: 2 [0/60000 (0%)]\tRecon: 1.880237\n",
            "Train Epoch: 2 [0/60000 (0%)]\tKL: 0.103736\n",
            "\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 1.774282\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tRecon: 1.716768\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tKL: 0.057514\n",
            "\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 1.656518\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tRecon: 1.612036\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tKL: 0.044482\n",
            "\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 1.684946\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tRecon: 1.654388\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tKL: 0.030558\n",
            "\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 1.717245\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tRecon: 1.633013\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tKL: 0.084231\n",
            "\n",
            "====> Epoch: 2 Average loss: 1.7947\n",
            "====> Test set loss: 19.8788\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 3.748806\n",
            "Train Epoch: 3 [0/60000 (0%)]\tRecon: 3.040219\n",
            "Train Epoch: 3 [0/60000 (0%)]\tKL: 0.708586\n",
            "\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 3.015221\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tRecon: 2.950380\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tKL: 0.064841\n",
            "\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 2.097205\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tRecon: 2.012318\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tKL: 0.084887\n",
            "\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 17.188784\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tRecon: 17.143921\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tKL: 0.044862\n",
            "\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 13.397843\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tRecon: 13.362667\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tKL: 0.035177\n",
            "\n",
            "====> Epoch: 3 Average loss: 9.9437\n",
            "====> Test set loss: 98.3073\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 17.028440\n",
            "Train Epoch: 4 [0/60000 (0%)]\tRecon: 17.013958\n",
            "Train Epoch: 4 [0/60000 (0%)]\tKL: 0.014482\n",
            "\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 16.102701\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tRecon: 16.071285\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tKL: 0.031416\n",
            "\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 2.034400\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tRecon: 1.777144\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tKL: 0.257256\n",
            "\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 19.998621\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tRecon: 19.966364\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tKL: 0.032257\n",
            "\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 4.023523\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tRecon: 3.920484\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tKL: 0.103039\n",
            "\n",
            "====> Epoch: 4 Average loss: 9.9557\n",
            "====> Test set loss: 12.6605\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 2.118456\n",
            "Train Epoch: 5 [0/60000 (0%)]\tRecon: 1.648507\n",
            "Train Epoch: 5 [0/60000 (0%)]\tKL: 0.469949\n",
            "\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 11.976848\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tRecon: 11.947161\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tKL: 0.029687\n",
            "\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 1.717613\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tRecon: 1.634303\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tKL: 0.083310\n",
            "\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 10.745109\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tRecon: 10.703434\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tKL: 0.041675\n",
            "\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 17.040098\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tRecon: 17.010569\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tKL: 0.029530\n",
            "\n",
            "====> Epoch: 5 Average loss: 11.0579\n",
            "====> Test set loss: 95.9289\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 15.702158\n",
            "Train Epoch: 6 [0/60000 (0%)]\tRecon: 15.681367\n",
            "Train Epoch: 6 [0/60000 (0%)]\tKL: 0.020791\n",
            "\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 19.806622\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tRecon: 19.788284\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tKL: 0.018337\n",
            "\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 10.978356\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tRecon: 10.926644\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tKL: 0.051713\n",
            "\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 4.082928\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tRecon: 4.012439\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tKL: 0.070489\n",
            "\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 5.244056\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tRecon: 5.157912\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tKL: 0.086144\n",
            "\n",
            "====> Epoch: 6 Average loss: 13.9789\n",
            "====> Test set loss: 48.9063\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 8.730380\n",
            "Train Epoch: 7 [0/60000 (0%)]\tRecon: 8.685093\n",
            "Train Epoch: 7 [0/60000 (0%)]\tKL: 0.045287\n",
            "\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 30.480085\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tRecon: 30.377293\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tKL: 0.102792\n",
            "\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 16.523987\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tRecon: 16.459042\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tKL: 0.064945\n",
            "\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 20.605011\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tRecon: 20.558645\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tKL: 0.046367\n",
            "\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 12.395341\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tRecon: 12.349504\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tKL: 0.045836\n",
            "\n",
            "====> Epoch: 7 Average loss: 16.6587\n",
            "====> Test set loss: 92.7288\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 16.497360\n",
            "Train Epoch: 8 [0/60000 (0%)]\tRecon: 16.461252\n",
            "Train Epoch: 8 [0/60000 (0%)]\tKL: 0.036108\n",
            "\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 19.700279\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tRecon: 19.651728\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tKL: 0.048552\n",
            "\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 28.324810\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tRecon: 28.184231\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tKL: 0.140580\n",
            "\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 2.153885\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tRecon: 1.832286\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tKL: 0.321599\n",
            "\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 16.980181\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tRecon: 16.909969\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tKL: 0.070212\n",
            "\n",
            "====> Epoch: 8 Average loss: 19.9690\n",
            "====> Test set loss: 107.0952\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 17.820068\n",
            "Train Epoch: 9 [0/60000 (0%)]\tRecon: 17.787701\n",
            "Train Epoch: 9 [0/60000 (0%)]\tKL: 0.032369\n",
            "\n",
            "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 29.015484\n",
            "Train Epoch: 9 [12800/60000 (21%)]\tRecon: 28.785227\n",
            "Train Epoch: 9 [12800/60000 (21%)]\tKL: 0.230256\n",
            "\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 27.118933\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tRecon: 26.871609\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tKL: 0.247324\n",
            "\n",
            "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 2.009858\n",
            "Train Epoch: 9 [38400/60000 (64%)]\tRecon: 1.633763\n",
            "Train Epoch: 9 [38400/60000 (64%)]\tKL: 0.376095\n",
            "\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 1.880531\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tRecon: 1.607150\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tKL: 0.273381\n",
            "\n",
            "====> Epoch: 9 Average loss: 13.4478\n",
            "====> Test set loss: 13.1737\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 2.097880\n",
            "Train Epoch: 10 [0/60000 (0%)]\tRecon: 1.589224\n",
            "Train Epoch: 10 [0/60000 (0%)]\tKL: 0.508657\n",
            "\n",
            "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 11.563443\n",
            "Train Epoch: 10 [12800/60000 (21%)]\tRecon: 11.506469\n",
            "Train Epoch: 10 [12800/60000 (21%)]\tKL: 0.056974\n",
            "\n",
            "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 26.102211\n",
            "Train Epoch: 10 [25600/60000 (43%)]\tRecon: 25.931765\n",
            "Train Epoch: 10 [25600/60000 (43%)]\tKL: 0.170446\n",
            "\n",
            "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 17.039347\n",
            "Train Epoch: 10 [38400/60000 (64%)]\tRecon: 16.954205\n",
            "Train Epoch: 10 [38400/60000 (64%)]\tKL: 0.085141\n",
            "\n",
            "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 27.586239\n",
            "Train Epoch: 10 [51200/60000 (85%)]\tRecon: 26.924847\n",
            "Train Epoch: 10 [51200/60000 (85%)]\tKL: 0.661393\n",
            "\n",
            "====> Epoch: 10 Average loss: 17.5829\n",
            "====> Test set loss: 179.5058\n",
            "Train Epoch: 11 [0/60000 (0%)]\tLoss: 30.956257\n",
            "Train Epoch: 11 [0/60000 (0%)]\tRecon: 29.947607\n",
            "Train Epoch: 11 [0/60000 (0%)]\tKL: 1.008649\n",
            "\n",
            "Train Epoch: 11 [12800/60000 (21%)]\tLoss: 6.072243\n",
            "Train Epoch: 11 [12800/60000 (21%)]\tRecon: 5.603405\n",
            "Train Epoch: 11 [12800/60000 (21%)]\tKL: 0.468839\n",
            "\n",
            "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 24.265278\n",
            "Train Epoch: 11 [25600/60000 (43%)]\tRecon: 24.001675\n",
            "Train Epoch: 11 [25600/60000 (43%)]\tKL: 0.263604\n",
            "\n",
            "Train Epoch: 11 [38400/60000 (64%)]\tLoss: 4.292317\n",
            "Train Epoch: 11 [38400/60000 (64%)]\tRecon: 4.065742\n",
            "Train Epoch: 11 [38400/60000 (64%)]\tKL: 0.226575\n",
            "\n",
            "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 16.796579\n",
            "Train Epoch: 11 [51200/60000 (85%)]\tRecon: 16.660847\n",
            "Train Epoch: 11 [51200/60000 (85%)]\tKL: 0.135732\n",
            "\n",
            "====> Epoch: 11 Average loss: 15.4364\n",
            "====> Test set loss: 25.0211\n",
            "Train Epoch: 12 [0/60000 (0%)]\tLoss: 3.758060\n",
            "Train Epoch: 12 [0/60000 (0%)]\tRecon: 3.478663\n",
            "Train Epoch: 12 [0/60000 (0%)]\tKL: 0.279396\n",
            "\n",
            "Train Epoch: 12 [12800/60000 (21%)]\tLoss: 6.662372\n",
            "Train Epoch: 12 [12800/60000 (21%)]\tRecon: 6.252863\n",
            "Train Epoch: 12 [12800/60000 (21%)]\tKL: 0.409509\n",
            "\n",
            "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 18.909370\n",
            "Train Epoch: 12 [25600/60000 (43%)]\tRecon: 18.715202\n",
            "Train Epoch: 12 [25600/60000 (43%)]\tKL: 0.194168\n",
            "\n",
            "Train Epoch: 12 [38400/60000 (64%)]\tLoss: 28.305304\n",
            "Train Epoch: 12 [38400/60000 (64%)]\tRecon: 27.989830\n",
            "Train Epoch: 12 [38400/60000 (64%)]\tKL: 0.315474\n",
            "\n",
            "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 10.631389\n",
            "Train Epoch: 12 [51200/60000 (85%)]\tRecon: 10.147480\n",
            "Train Epoch: 12 [51200/60000 (85%)]\tKL: 0.483909\n",
            "\n",
            "====> Epoch: 12 Average loss: 15.5048\n",
            "====> Test set loss: 160.0369\n",
            "Train Epoch: 13 [0/60000 (0%)]\tLoss: 26.451527\n",
            "Train Epoch: 13 [0/60000 (0%)]\tRecon: 25.754614\n",
            "Train Epoch: 13 [0/60000 (0%)]\tKL: 0.696914\n",
            "\n",
            "Train Epoch: 13 [12800/60000 (21%)]\tLoss: 11.479630\n",
            "Train Epoch: 13 [12800/60000 (21%)]\tRecon: 11.216473\n",
            "Train Epoch: 13 [12800/60000 (21%)]\tKL: 0.263157\n",
            "\n",
            "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 19.432861\n",
            "Train Epoch: 13 [25600/60000 (43%)]\tRecon: 19.303606\n",
            "Train Epoch: 13 [25600/60000 (43%)]\tKL: 0.129256\n",
            "\n",
            "Train Epoch: 13 [38400/60000 (64%)]\tLoss: 30.918615\n",
            "Train Epoch: 13 [38400/60000 (64%)]\tRecon: 29.728567\n",
            "Train Epoch: 13 [38400/60000 (64%)]\tKL: 1.190049\n",
            "\n",
            "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 6.649493\n",
            "Train Epoch: 13 [51200/60000 (85%)]\tRecon: 6.060612\n",
            "Train Epoch: 13 [51200/60000 (85%)]\tKL: 0.588881\n",
            "\n",
            "====> Epoch: 13 Average loss: 17.5101\n",
            "====> Test set loss: 17.2657\n",
            "Train Epoch: 14 [0/60000 (0%)]\tLoss: 2.739332\n",
            "Train Epoch: 14 [0/60000 (0%)]\tRecon: 1.655588\n",
            "Train Epoch: 14 [0/60000 (0%)]\tKL: 1.083744\n",
            "\n",
            "Train Epoch: 14 [12800/60000 (21%)]\tLoss: 4.834507\n",
            "Train Epoch: 14 [12800/60000 (21%)]\tRecon: 1.676068\n",
            "Train Epoch: 14 [12800/60000 (21%)]\tKL: 3.158439\n",
            "\n",
            "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 16.586819\n",
            "Train Epoch: 14 [25600/60000 (43%)]\tRecon: 16.066822\n",
            "Train Epoch: 14 [25600/60000 (43%)]\tKL: 0.519997\n",
            "\n",
            "Train Epoch: 14 [38400/60000 (64%)]\tLoss: 15.152599\n",
            "Train Epoch: 14 [38400/60000 (64%)]\tRecon: 14.863380\n",
            "Train Epoch: 14 [38400/60000 (64%)]\tKL: 0.289218\n",
            "\n",
            "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 30.908667\n",
            "Train Epoch: 14 [51200/60000 (85%)]\tRecon: 29.403746\n",
            "Train Epoch: 14 [51200/60000 (85%)]\tKL: 1.504922\n",
            "\n",
            "====> Epoch: 14 Average loss: 18.6279\n",
            "====> Test set loss: 22.4786\n",
            "Train Epoch: 15 [0/60000 (0%)]\tLoss: 3.747862\n",
            "Train Epoch: 15 [0/60000 (0%)]\tRecon: 2.086221\n",
            "Train Epoch: 15 [0/60000 (0%)]\tKL: 1.661641\n",
            "\n",
            "Train Epoch: 15 [12800/60000 (21%)]\tLoss: 14.033776\n",
            "Train Epoch: 15 [12800/60000 (21%)]\tRecon: 13.740808\n",
            "Train Epoch: 15 [12800/60000 (21%)]\tKL: 0.292969\n",
            "\n",
            "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 14.396124\n",
            "Train Epoch: 15 [25600/60000 (43%)]\tRecon: 14.051537\n",
            "Train Epoch: 15 [25600/60000 (43%)]\tKL: 0.344587\n",
            "\n",
            "Train Epoch: 15 [38400/60000 (64%)]\tLoss: 19.489382\n",
            "Train Epoch: 15 [38400/60000 (64%)]\tRecon: 19.051611\n",
            "Train Epoch: 15 [38400/60000 (64%)]\tKL: 0.437772\n",
            "\n",
            "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 7.964514\n",
            "Train Epoch: 15 [51200/60000 (85%)]\tRecon: 1.649814\n",
            "Train Epoch: 15 [51200/60000 (85%)]\tKL: 6.314700\n",
            "\n",
            "====> Epoch: 15 Average loss: 13.9326\n",
            "====> Test set loss: 212.5292\n",
            "Train Epoch: 16 [0/60000 (0%)]\tLoss: 35.796944\n",
            "Train Epoch: 16 [0/60000 (0%)]\tRecon: 29.190319\n",
            "Train Epoch: 16 [0/60000 (0%)]\tKL: 6.606623\n",
            "\n",
            "Train Epoch: 16 [12800/60000 (21%)]\tLoss: 11.552892\n",
            "Train Epoch: 16 [12800/60000 (21%)]\tRecon: 8.416614\n",
            "Train Epoch: 16 [12800/60000 (21%)]\tKL: 3.136278\n",
            "\n",
            "Train Epoch: 16 [25600/60000 (43%)]\tLoss: 20.766695\n",
            "Train Epoch: 16 [25600/60000 (43%)]\tRecon: 17.980206\n",
            "Train Epoch: 16 [25600/60000 (43%)]\tKL: 2.786489\n",
            "\n",
            "Train Epoch: 16 [38400/60000 (64%)]\tLoss: 19.917997\n",
            "Train Epoch: 16 [38400/60000 (64%)]\tRecon: 19.505577\n",
            "Train Epoch: 16 [38400/60000 (64%)]\tKL: 0.412420\n",
            "\n",
            "Train Epoch: 16 [51200/60000 (85%)]\tLoss: 27.623318\n",
            "Train Epoch: 16 [51200/60000 (85%)]\tRecon: 23.088289\n",
            "Train Epoch: 16 [51200/60000 (85%)]\tKL: 4.535029\n",
            "\n",
            "====> Epoch: 16 Average loss: 21.9971\n",
            "====> Test set loss: 87.9630\n",
            "Train Epoch: 17 [0/60000 (0%)]\tLoss: 13.932326\n",
            "Train Epoch: 17 [0/60000 (0%)]\tRecon: 13.177747\n",
            "Train Epoch: 17 [0/60000 (0%)]\tKL: 0.754579\n",
            "\n",
            "Train Epoch: 17 [12800/60000 (21%)]\tLoss: 13.511988\n",
            "Train Epoch: 17 [12800/60000 (21%)]\tRecon: 13.192787\n",
            "Train Epoch: 17 [12800/60000 (21%)]\tKL: 0.319201\n",
            "\n",
            "Train Epoch: 17 [25600/60000 (43%)]\tLoss: 17.957890\n",
            "Train Epoch: 17 [25600/60000 (43%)]\tRecon: 17.687754\n",
            "Train Epoch: 17 [25600/60000 (43%)]\tKL: 0.270136\n",
            "\n",
            "Train Epoch: 17 [38400/60000 (64%)]\tLoss: 18.315542\n",
            "Train Epoch: 17 [38400/60000 (64%)]\tRecon: 17.521530\n",
            "Train Epoch: 17 [38400/60000 (64%)]\tKL: 0.794013\n",
            "\n",
            "Train Epoch: 17 [51200/60000 (85%)]\tLoss: 14.891422\n",
            "Train Epoch: 17 [51200/60000 (85%)]\tRecon: 14.472115\n",
            "Train Epoch: 17 [51200/60000 (85%)]\tKL: 0.419308\n",
            "\n",
            "====> Epoch: 17 Average loss: 16.3338\n",
            "====> Test set loss: 115.1921\n",
            "Train Epoch: 18 [0/60000 (0%)]\tLoss: 17.693531\n",
            "Train Epoch: 18 [0/60000 (0%)]\tRecon: 17.432989\n",
            "Train Epoch: 18 [0/60000 (0%)]\tKL: 0.260541\n",
            "\n",
            "Train Epoch: 18 [12800/60000 (21%)]\tLoss: 18.336559\n",
            "Train Epoch: 18 [12800/60000 (21%)]\tRecon: 18.065613\n",
            "Train Epoch: 18 [12800/60000 (21%)]\tKL: 0.270946\n",
            "\n",
            "Train Epoch: 18 [25600/60000 (43%)]\tLoss: 11.338925\n",
            "Train Epoch: 18 [25600/60000 (43%)]\tRecon: 11.112689\n",
            "Train Epoch: 18 [25600/60000 (43%)]\tKL: 0.226237\n",
            "\n",
            "Train Epoch: 18 [38400/60000 (64%)]\tLoss: 19.724062\n",
            "Train Epoch: 18 [38400/60000 (64%)]\tRecon: 19.541464\n",
            "Train Epoch: 18 [38400/60000 (64%)]\tKL: 0.182598\n",
            "\n",
            "Train Epoch: 18 [51200/60000 (85%)]\tLoss: 19.129707\n",
            "Train Epoch: 18 [51200/60000 (85%)]\tRecon: 18.913958\n",
            "Train Epoch: 18 [51200/60000 (85%)]\tKL: 0.215750\n",
            "\n",
            "====> Epoch: 18 Average loss: 15.9787\n",
            "====> Test set loss: 107.7981\n",
            "Train Epoch: 19 [0/60000 (0%)]\tLoss: 19.534489\n",
            "Train Epoch: 19 [0/60000 (0%)]\tRecon: 19.372856\n",
            "Train Epoch: 19 [0/60000 (0%)]\tKL: 0.161633\n",
            "\n",
            "Train Epoch: 19 [12800/60000 (21%)]\tLoss: 21.970585\n",
            "Train Epoch: 19 [12800/60000 (21%)]\tRecon: 5.634638\n",
            "Train Epoch: 19 [12800/60000 (21%)]\tKL: 16.335947\n",
            "\n",
            "Train Epoch: 19 [25600/60000 (43%)]\tLoss: 16.732998\n",
            "Train Epoch: 19 [25600/60000 (43%)]\tRecon: 16.088482\n",
            "Train Epoch: 19 [25600/60000 (43%)]\tKL: 0.644516\n",
            "\n",
            "Train Epoch: 19 [38400/60000 (64%)]\tLoss: 9.912009\n",
            "Train Epoch: 19 [38400/60000 (64%)]\tRecon: 9.548299\n",
            "Train Epoch: 19 [38400/60000 (64%)]\tKL: 0.363710\n",
            "\n",
            "Train Epoch: 19 [51200/60000 (85%)]\tLoss: 28.021170\n",
            "Train Epoch: 19 [51200/60000 (85%)]\tRecon: 23.977579\n",
            "Train Epoch: 19 [51200/60000 (85%)]\tKL: 4.043591\n",
            "\n",
            "====> Epoch: 19 Average loss: 21.5545\n",
            "====> Test set loss: 88.7397\n",
            "Train Epoch: 20 [0/60000 (0%)]\tLoss: 15.009076\n",
            "Train Epoch: 20 [0/60000 (0%)]\tRecon: 13.822845\n",
            "Train Epoch: 20 [0/60000 (0%)]\tKL: 1.186232\n",
            "\n",
            "Train Epoch: 20 [12800/60000 (21%)]\tLoss: 18.300808\n",
            "Train Epoch: 20 [12800/60000 (21%)]\tRecon: 17.822140\n",
            "Train Epoch: 20 [12800/60000 (21%)]\tKL: 0.478669\n",
            "\n",
            "Train Epoch: 20 [25600/60000 (43%)]\tLoss: 13.326809\n",
            "Train Epoch: 20 [25600/60000 (43%)]\tRecon: 12.943748\n",
            "Train Epoch: 20 [25600/60000 (43%)]\tKL: 0.383060\n",
            "\n",
            "Train Epoch: 20 [38400/60000 (64%)]\tLoss: 18.625872\n",
            "Train Epoch: 20 [38400/60000 (64%)]\tRecon: 17.560154\n",
            "Train Epoch: 20 [38400/60000 (64%)]\tKL: 1.065717\n",
            "\n",
            "Train Epoch: 20 [51200/60000 (85%)]\tLoss: 12.591255\n",
            "Train Epoch: 20 [51200/60000 (85%)]\tRecon: 12.231316\n",
            "Train Epoch: 20 [51200/60000 (85%)]\tKL: 0.359939\n",
            "\n",
            "====> Epoch: 20 Average loss: 15.0051\n",
            "====> Test set loss: 89.4419\n",
            "Train Epoch: 21 [0/60000 (0%)]\tLoss: 15.070534\n",
            "Train Epoch: 21 [0/60000 (0%)]\tRecon: 14.749065\n",
            "Train Epoch: 21 [0/60000 (0%)]\tKL: 0.321469\n",
            "\n",
            "Train Epoch: 21 [12800/60000 (21%)]\tLoss: 14.594086\n",
            "Train Epoch: 21 [12800/60000 (21%)]\tRecon: 14.303433\n",
            "Train Epoch: 21 [12800/60000 (21%)]\tKL: 0.290652\n",
            "\n",
            "Train Epoch: 21 [25600/60000 (43%)]\tLoss: 18.443895\n",
            "Train Epoch: 21 [25600/60000 (43%)]\tRecon: 18.156492\n",
            "Train Epoch: 21 [25600/60000 (43%)]\tKL: 0.287403\n",
            "\n",
            "Train Epoch: 21 [38400/60000 (64%)]\tLoss: 17.417294\n",
            "Train Epoch: 21 [38400/60000 (64%)]\tRecon: 17.152805\n",
            "Train Epoch: 21 [38400/60000 (64%)]\tKL: 0.264488\n",
            "\n",
            "Train Epoch: 21 [51200/60000 (85%)]\tLoss: 16.984549\n",
            "Train Epoch: 21 [51200/60000 (85%)]\tRecon: 16.744274\n",
            "Train Epoch: 21 [51200/60000 (85%)]\tKL: 0.240275\n",
            "\n",
            "====> Epoch: 21 Average loss: 15.5271\n",
            "====> Test set loss: 78.1143\n",
            "Train Epoch: 22 [0/60000 (0%)]\tLoss: 12.243528\n",
            "Train Epoch: 22 [0/60000 (0%)]\tRecon: 12.024002\n",
            "Train Epoch: 22 [0/60000 (0%)]\tKL: 0.219526\n",
            "\n",
            "Train Epoch: 22 [12800/60000 (21%)]\tLoss: 82.514572\n",
            "Train Epoch: 22 [12800/60000 (21%)]\tRecon: 1.745890\n",
            "Train Epoch: 22 [12800/60000 (21%)]\tKL: 80.768684\n",
            "\n",
            "Train Epoch: 22 [25600/60000 (43%)]\tLoss: 16.645391\n",
            "Train Epoch: 22 [25600/60000 (43%)]\tRecon: 15.137217\n",
            "Train Epoch: 22 [25600/60000 (43%)]\tKL: 1.508174\n",
            "\n",
            "Train Epoch: 22 [38400/60000 (64%)]\tLoss: 17.365900\n",
            "Train Epoch: 22 [38400/60000 (64%)]\tRecon: 16.650211\n",
            "Train Epoch: 22 [38400/60000 (64%)]\tKL: 0.715688\n",
            "\n",
            "Train Epoch: 22 [51200/60000 (85%)]\tLoss: 21.457697\n",
            "Train Epoch: 22 [51200/60000 (85%)]\tRecon: 20.893818\n",
            "Train Epoch: 22 [51200/60000 (85%)]\tKL: 0.563879\n",
            "\n",
            "====> Epoch: 22 Average loss: 22.7807\n",
            "====> Test set loss: 90.2047\n",
            "Train Epoch: 23 [0/60000 (0%)]\tLoss: 13.432723\n",
            "Train Epoch: 23 [0/60000 (0%)]\tRecon: 13.020449\n",
            "Train Epoch: 23 [0/60000 (0%)]\tKL: 0.412274\n",
            "\n",
            "Train Epoch: 23 [12800/60000 (21%)]\tLoss: 9.151827\n",
            "Train Epoch: 23 [12800/60000 (21%)]\tRecon: 8.814312\n",
            "Train Epoch: 23 [12800/60000 (21%)]\tKL: 0.337515\n",
            "\n",
            "Train Epoch: 23 [25600/60000 (43%)]\tLoss: 25.867210\n",
            "Train Epoch: 23 [25600/60000 (43%)]\tRecon: 17.425575\n",
            "Train Epoch: 23 [25600/60000 (43%)]\tKL: 8.441635\n",
            "\n",
            "Train Epoch: 23 [38400/60000 (64%)]\tLoss: 21.593296\n",
            "Train Epoch: 23 [38400/60000 (64%)]\tRecon: 19.253632\n",
            "Train Epoch: 23 [38400/60000 (64%)]\tKL: 2.339664\n",
            "\n",
            "Train Epoch: 23 [51200/60000 (85%)]\tLoss: 5.737157\n",
            "Train Epoch: 23 [51200/60000 (85%)]\tRecon: 4.056738\n",
            "Train Epoch: 23 [51200/60000 (85%)]\tKL: 1.680419\n",
            "\n",
            "====> Epoch: 23 Average loss: 42.8413\n",
            "====> Test set loss: 90.7256\n",
            "Train Epoch: 24 [0/60000 (0%)]\tLoss: 12.823121\n",
            "Train Epoch: 24 [0/60000 (0%)]\tRecon: 11.717201\n",
            "Train Epoch: 24 [0/60000 (0%)]\tKL: 1.105920\n",
            "\n",
            "Train Epoch: 24 [12800/60000 (21%)]\tLoss: 17.018621\n",
            "Train Epoch: 24 [12800/60000 (21%)]\tRecon: 16.172909\n",
            "Train Epoch: 24 [12800/60000 (21%)]\tKL: 0.845713\n",
            "\n",
            "Train Epoch: 24 [25600/60000 (43%)]\tLoss: 68.181015\n",
            "Train Epoch: 24 [25600/60000 (43%)]\tRecon: 28.842365\n",
            "Train Epoch: 24 [25600/60000 (43%)]\tKL: 39.338654\n",
            "\n",
            "Train Epoch: 24 [38400/60000 (64%)]\tLoss: 26.631205\n",
            "Train Epoch: 24 [38400/60000 (64%)]\tRecon: 21.333389\n",
            "Train Epoch: 24 [38400/60000 (64%)]\tKL: 5.297815\n",
            "\n",
            "Train Epoch: 24 [51200/60000 (85%)]\tLoss: 17.118500\n",
            "Train Epoch: 24 [51200/60000 (85%)]\tRecon: 14.847453\n",
            "Train Epoch: 24 [51200/60000 (85%)]\tKL: 2.271046\n",
            "\n",
            "====> Epoch: 24 Average loss: 27.7626\n",
            "====> Test set loss: 126.4209\n",
            "Train Epoch: 25 [0/60000 (0%)]\tLoss: 21.049183\n",
            "Train Epoch: 25 [0/60000 (0%)]\tRecon: 18.875467\n",
            "Train Epoch: 25 [0/60000 (0%)]\tKL: 2.173716\n",
            "\n",
            "Train Epoch: 25 [12800/60000 (21%)]\tLoss: 9.370224\n",
            "Train Epoch: 25 [12800/60000 (21%)]\tRecon: 7.796554\n",
            "Train Epoch: 25 [12800/60000 (21%)]\tKL: 1.573670\n",
            "\n",
            "Train Epoch: 25 [25600/60000 (43%)]\tLoss: 27.293070\n",
            "Train Epoch: 25 [25600/60000 (43%)]\tRecon: 25.057062\n",
            "Train Epoch: 25 [25600/60000 (43%)]\tKL: 2.236008\n",
            "\n",
            "Train Epoch: 25 [38400/60000 (64%)]\tLoss: 29.539604\n",
            "Train Epoch: 25 [38400/60000 (64%)]\tRecon: 19.223896\n",
            "Train Epoch: 25 [38400/60000 (64%)]\tKL: 10.315708\n",
            "\n",
            "Train Epoch: 25 [51200/60000 (85%)]\tLoss: 21.289457\n",
            "Train Epoch: 25 [51200/60000 (85%)]\tRecon: 19.125494\n",
            "Train Epoch: 25 [51200/60000 (85%)]\tKL: 2.163963\n",
            "\n",
            "====> Epoch: 25 Average loss: 32.3782\n",
            "====> Test set loss: 77.3248\n",
            "Train Epoch: 26 [0/60000 (0%)]\tLoss: 12.908971\n",
            "Train Epoch: 26 [0/60000 (0%)]\tRecon: 10.979140\n",
            "Train Epoch: 26 [0/60000 (0%)]\tKL: 1.929830\n",
            "\n",
            "Train Epoch: 26 [12800/60000 (21%)]\tLoss: 15.818562\n",
            "Train Epoch: 26 [12800/60000 (21%)]\tRecon: 14.285238\n",
            "Train Epoch: 26 [12800/60000 (21%)]\tKL: 1.533323\n",
            "\n",
            "Train Epoch: 26 [25600/60000 (43%)]\tLoss: 17.375845\n",
            "Train Epoch: 26 [25600/60000 (43%)]\tRecon: 14.637711\n",
            "Train Epoch: 26 [25600/60000 (43%)]\tKL: 2.738135\n",
            "\n",
            "Train Epoch: 26 [38400/60000 (64%)]\tLoss: 12.922096\n",
            "Train Epoch: 26 [38400/60000 (64%)]\tRecon: 11.367476\n",
            "Train Epoch: 26 [38400/60000 (64%)]\tKL: 1.554620\n",
            "\n",
            "Train Epoch: 26 [51200/60000 (85%)]\tLoss: 14.730161\n",
            "Train Epoch: 26 [51200/60000 (85%)]\tRecon: 13.627239\n",
            "Train Epoch: 26 [51200/60000 (85%)]\tKL: 1.102922\n",
            "\n",
            "====> Epoch: 26 Average loss: 18.0161\n",
            "====> Test set loss: 134.5151\n",
            "Train Epoch: 27 [0/60000 (0%)]\tLoss: 20.641645\n",
            "Train Epoch: 27 [0/60000 (0%)]\tRecon: 19.483490\n",
            "Train Epoch: 27 [0/60000 (0%)]\tKL: 1.158155\n",
            "\n",
            "Train Epoch: 27 [12800/60000 (21%)]\tLoss: 14.460299\n",
            "Train Epoch: 27 [12800/60000 (21%)]\tRecon: 13.500940\n",
            "Train Epoch: 27 [12800/60000 (21%)]\tKL: 0.959358\n",
            "\n",
            "Train Epoch: 27 [25600/60000 (43%)]\tLoss: 13.873227\n",
            "Train Epoch: 27 [25600/60000 (43%)]\tRecon: 12.960762\n",
            "Train Epoch: 27 [25600/60000 (43%)]\tKL: 0.912465\n",
            "\n",
            "Train Epoch: 27 [38400/60000 (64%)]\tLoss: 15.004826\n",
            "Train Epoch: 27 [38400/60000 (64%)]\tRecon: 14.207738\n",
            "Train Epoch: 27 [38400/60000 (64%)]\tKL: 0.797088\n",
            "\n",
            "Train Epoch: 27 [51200/60000 (85%)]\tLoss: 9.136874\n",
            "Train Epoch: 27 [51200/60000 (85%)]\tRecon: 8.174927\n",
            "Train Epoch: 27 [51200/60000 (85%)]\tKL: 0.961947\n",
            "\n",
            "====> Epoch: 27 Average loss: 15.9195\n",
            "====> Test set loss: 124.8053\n",
            "Train Epoch: 28 [0/60000 (0%)]\tLoss: 20.313347\n",
            "Train Epoch: 28 [0/60000 (0%)]\tRecon: 19.463432\n",
            "Train Epoch: 28 [0/60000 (0%)]\tKL: 0.849914\n",
            "\n",
            "Train Epoch: 28 [12800/60000 (21%)]\tLoss: 19.367098\n",
            "Train Epoch: 28 [12800/60000 (21%)]\tRecon: 18.574835\n",
            "Train Epoch: 28 [12800/60000 (21%)]\tKL: 0.792264\n",
            "\n",
            "Train Epoch: 28 [25600/60000 (43%)]\tLoss: 11.363718\n",
            "Train Epoch: 28 [25600/60000 (43%)]\tRecon: 10.628059\n",
            "Train Epoch: 28 [25600/60000 (43%)]\tKL: 0.735659\n",
            "\n",
            "Train Epoch: 28 [38400/60000 (64%)]\tLoss: 17.052494\n",
            "Train Epoch: 28 [38400/60000 (64%)]\tRecon: 16.234951\n",
            "Train Epoch: 28 [38400/60000 (64%)]\tKL: 0.817542\n",
            "\n",
            "Train Epoch: 28 [51200/60000 (85%)]\tLoss: 16.501179\n",
            "Train Epoch: 28 [51200/60000 (85%)]\tRecon: 15.802767\n",
            "Train Epoch: 28 [51200/60000 (85%)]\tKL: 0.698411\n",
            "\n",
            "====> Epoch: 28 Average loss: 16.1727\n",
            "====> Test set loss: 74.9003\n",
            "Train Epoch: 29 [0/60000 (0%)]\tLoss: 11.466609\n",
            "Train Epoch: 29 [0/60000 (0%)]\tRecon: 10.752149\n",
            "Train Epoch: 29 [0/60000 (0%)]\tKL: 0.714460\n",
            "\n",
            "Train Epoch: 29 [12800/60000 (21%)]\tLoss: 19.520735\n",
            "Train Epoch: 29 [12800/60000 (21%)]\tRecon: 18.866001\n",
            "Train Epoch: 29 [12800/60000 (21%)]\tKL: 0.654733\n",
            "\n",
            "Train Epoch: 29 [25600/60000 (43%)]\tLoss: 9.756698\n",
            "Train Epoch: 29 [25600/60000 (43%)]\tRecon: 9.064754\n",
            "Train Epoch: 29 [25600/60000 (43%)]\tKL: 0.691944\n",
            "\n",
            "Train Epoch: 29 [38400/60000 (64%)]\tLoss: 17.104626\n",
            "Train Epoch: 29 [38400/60000 (64%)]\tRecon: 16.336205\n",
            "Train Epoch: 29 [38400/60000 (64%)]\tKL: 0.768421\n",
            "\n",
            "Train Epoch: 29 [51200/60000 (85%)]\tLoss: 13.149322\n",
            "Train Epoch: 29 [51200/60000 (85%)]\tRecon: 12.540330\n",
            "Train Epoch: 29 [51200/60000 (85%)]\tKL: 0.608992\n",
            "\n",
            "====> Epoch: 29 Average loss: 15.5681\n",
            "====> Test set loss: 148.2929\n",
            "Train Epoch: 30 [0/60000 (0%)]\tLoss: 25.324627\n",
            "Train Epoch: 30 [0/60000 (0%)]\tRecon: 24.577127\n",
            "Train Epoch: 30 [0/60000 (0%)]\tKL: 0.747499\n",
            "\n",
            "Train Epoch: 30 [12800/60000 (21%)]\tLoss: 10.916198\n",
            "Train Epoch: 30 [12800/60000 (21%)]\tRecon: 10.218264\n",
            "Train Epoch: 30 [12800/60000 (21%)]\tKL: 0.697934\n",
            "\n",
            "Train Epoch: 30 [25600/60000 (43%)]\tLoss: 20.639339\n",
            "Train Epoch: 30 [25600/60000 (43%)]\tRecon: 20.039558\n",
            "Train Epoch: 30 [25600/60000 (43%)]\tKL: 0.599782\n",
            "\n",
            "Train Epoch: 30 [38400/60000 (64%)]\tLoss: 6.264833\n",
            "Train Epoch: 30 [38400/60000 (64%)]\tRecon: 5.546765\n",
            "Train Epoch: 30 [38400/60000 (64%)]\tKL: 0.718068\n",
            "\n",
            "Train Epoch: 30 [51200/60000 (85%)]\tLoss: 12.984557\n",
            "Train Epoch: 30 [51200/60000 (85%)]\tRecon: 12.374988\n",
            "Train Epoch: 30 [51200/60000 (85%)]\tKL: 0.609570\n",
            "\n",
            "====> Epoch: 30 Average loss: 15.9605\n",
            "====> Test set loss: 115.3806\n",
            "Train Epoch: 31 [0/60000 (0%)]\tLoss: 18.179384\n",
            "Train Epoch: 31 [0/60000 (0%)]\tRecon: 17.570370\n",
            "Train Epoch: 31 [0/60000 (0%)]\tKL: 0.609015\n",
            "\n",
            "Train Epoch: 31 [12800/60000 (21%)]\tLoss: 10.465304\n",
            "Train Epoch: 31 [12800/60000 (21%)]\tRecon: 9.902034\n",
            "Train Epoch: 31 [12800/60000 (21%)]\tKL: 0.563270\n",
            "\n",
            "Train Epoch: 31 [25600/60000 (43%)]\tLoss: 17.643337\n",
            "Train Epoch: 31 [25600/60000 (43%)]\tRecon: 17.127731\n",
            "Train Epoch: 31 [25600/60000 (43%)]\tKL: 0.515607\n",
            "\n",
            "Train Epoch: 31 [38400/60000 (64%)]\tLoss: 20.506804\n",
            "Train Epoch: 31 [38400/60000 (64%)]\tRecon: 20.000492\n",
            "Train Epoch: 31 [38400/60000 (64%)]\tKL: 0.506311\n",
            "\n",
            "Train Epoch: 31 [51200/60000 (85%)]\tLoss: 7.597890\n",
            "Train Epoch: 31 [51200/60000 (85%)]\tRecon: 6.935715\n",
            "Train Epoch: 31 [51200/60000 (85%)]\tKL: 0.662175\n",
            "\n",
            "====> Epoch: 31 Average loss: 16.0939\n",
            "====> Test set loss: 126.6059\n",
            "Train Epoch: 32 [0/60000 (0%)]\tLoss: 22.197821\n",
            "Train Epoch: 32 [0/60000 (0%)]\tRecon: 21.612585\n",
            "Train Epoch: 32 [0/60000 (0%)]\tKL: 0.585236\n",
            "\n",
            "Train Epoch: 32 [12800/60000 (21%)]\tLoss: 17.316341\n",
            "Train Epoch: 32 [12800/60000 (21%)]\tRecon: 16.856720\n",
            "Train Epoch: 32 [12800/60000 (21%)]\tKL: 0.459622\n",
            "\n",
            "Train Epoch: 32 [25600/60000 (43%)]\tLoss: 12.979585\n",
            "Train Epoch: 32 [25600/60000 (43%)]\tRecon: 12.434952\n",
            "Train Epoch: 32 [25600/60000 (43%)]\tKL: 0.544633\n",
            "\n",
            "Train Epoch: 32 [38400/60000 (64%)]\tLoss: 14.817750\n",
            "Train Epoch: 32 [38400/60000 (64%)]\tRecon: 14.335031\n",
            "Train Epoch: 32 [38400/60000 (64%)]\tKL: 0.482719\n",
            "\n",
            "Train Epoch: 32 [51200/60000 (85%)]\tLoss: 7.522250\n",
            "Train Epoch: 32 [51200/60000 (85%)]\tRecon: 7.041748\n",
            "Train Epoch: 32 [51200/60000 (85%)]\tKL: 0.480503\n",
            "\n",
            "====> Epoch: 32 Average loss: 15.2354\n",
            "====> Test set loss: 80.2980\n",
            "Train Epoch: 33 [0/60000 (0%)]\tLoss: 13.183892\n",
            "Train Epoch: 33 [0/60000 (0%)]\tRecon: 12.592331\n",
            "Train Epoch: 33 [0/60000 (0%)]\tKL: 0.591561\n",
            "\n",
            "Train Epoch: 33 [12800/60000 (21%)]\tLoss: 137.559113\n",
            "Train Epoch: 33 [12800/60000 (21%)]\tRecon: 8.392996\n",
            "Train Epoch: 33 [12800/60000 (21%)]\tKL: 129.166122\n",
            "\n",
            "Train Epoch: 33 [25600/60000 (43%)]\tLoss: 33.074760\n",
            "Train Epoch: 33 [25600/60000 (43%)]\tRecon: 18.550472\n",
            "Train Epoch: 33 [25600/60000 (43%)]\tKL: 14.524286\n",
            "\n",
            "Train Epoch: 33 [38400/60000 (64%)]\tLoss: 22.474838\n",
            "Train Epoch: 33 [38400/60000 (64%)]\tRecon: 15.646748\n",
            "Train Epoch: 33 [38400/60000 (64%)]\tKL: 6.828090\n",
            "\n",
            "Train Epoch: 33 [51200/60000 (85%)]\tLoss: 20.385178\n",
            "Train Epoch: 33 [51200/60000 (85%)]\tRecon: 15.858524\n",
            "Train Epoch: 33 [51200/60000 (85%)]\tKL: 4.526652\n",
            "\n",
            "====> Epoch: 33 Average loss: 115.7000\n",
            "====> Test set loss: 130.9225\n",
            "Train Epoch: 34 [0/60000 (0%)]\tLoss: 22.562408\n",
            "Train Epoch: 34 [0/60000 (0%)]\tRecon: 18.433609\n",
            "Train Epoch: 34 [0/60000 (0%)]\tKL: 4.128800\n",
            "\n",
            "Train Epoch: 34 [12800/60000 (21%)]\tLoss: 27.653244\n",
            "Train Epoch: 34 [12800/60000 (21%)]\tRecon: 24.031710\n",
            "Train Epoch: 34 [12800/60000 (21%)]\tKL: 3.621534\n",
            "\n",
            "Train Epoch: 34 [25600/60000 (43%)]\tLoss: 21.647335\n",
            "Train Epoch: 34 [25600/60000 (43%)]\tRecon: 18.758909\n",
            "Train Epoch: 34 [25600/60000 (43%)]\tKL: 2.888425\n",
            "\n",
            "Train Epoch: 34 [38400/60000 (64%)]\tLoss: 13.496892\n",
            "Train Epoch: 34 [38400/60000 (64%)]\tRecon: 10.761347\n",
            "Train Epoch: 34 [38400/60000 (64%)]\tKL: 2.735545\n",
            "\n",
            "Train Epoch: 34 [51200/60000 (85%)]\tLoss: 23.794605\n",
            "Train Epoch: 34 [51200/60000 (85%)]\tRecon: 21.386623\n",
            "Train Epoch: 34 [51200/60000 (85%)]\tKL: 2.407982\n",
            "\n",
            "====> Epoch: 34 Average loss: 20.2940\n",
            "====> Test set loss: 109.3401\n",
            "Train Epoch: 35 [0/60000 (0%)]\tLoss: 20.478146\n",
            "Train Epoch: 35 [0/60000 (0%)]\tRecon: 16.911474\n",
            "Train Epoch: 35 [0/60000 (0%)]\tKL: 3.566672\n",
            "\n",
            "Train Epoch: 35 [12800/60000 (21%)]\tLoss: 11.449899\n",
            "Train Epoch: 35 [12800/60000 (21%)]\tRecon: 9.331147\n",
            "Train Epoch: 35 [12800/60000 (21%)]\tKL: 2.118751\n",
            "\n",
            "Train Epoch: 35 [25600/60000 (43%)]\tLoss: 15.971201\n",
            "Train Epoch: 35 [25600/60000 (43%)]\tRecon: 14.032631\n",
            "Train Epoch: 35 [25600/60000 (43%)]\tKL: 1.938570\n",
            "\n",
            "Train Epoch: 35 [38400/60000 (64%)]\tLoss: 25.304144\n",
            "Train Epoch: 35 [38400/60000 (64%)]\tRecon: 23.384186\n",
            "Train Epoch: 35 [38400/60000 (64%)]\tKL: 1.919959\n",
            "\n",
            "Train Epoch: 35 [51200/60000 (85%)]\tLoss: 17.535515\n",
            "Train Epoch: 35 [51200/60000 (85%)]\tRecon: 15.737084\n",
            "Train Epoch: 35 [51200/60000 (85%)]\tKL: 1.798430\n",
            "\n",
            "====> Epoch: 35 Average loss: 17.6566\n",
            "====> Test set loss: 125.7653\n",
            "Train Epoch: 36 [0/60000 (0%)]\tLoss: 22.889868\n",
            "Train Epoch: 36 [0/60000 (0%)]\tRecon: 21.305540\n",
            "Train Epoch: 36 [0/60000 (0%)]\tKL: 1.584327\n",
            "\n",
            "Train Epoch: 36 [12800/60000 (21%)]\tLoss: 20.640549\n",
            "Train Epoch: 36 [12800/60000 (21%)]\tRecon: 19.143417\n",
            "Train Epoch: 36 [12800/60000 (21%)]\tKL: 1.497132\n",
            "\n",
            "Train Epoch: 36 [25600/60000 (43%)]\tLoss: 9.681758\n",
            "Train Epoch: 36 [25600/60000 (43%)]\tRecon: 8.175985\n",
            "Train Epoch: 36 [25600/60000 (43%)]\tKL: 1.505773\n",
            "\n",
            "Train Epoch: 36 [38400/60000 (64%)]\tLoss: 12.495329\n",
            "Train Epoch: 36 [38400/60000 (64%)]\tRecon: 11.011868\n",
            "Train Epoch: 36 [38400/60000 (64%)]\tKL: 1.483461\n",
            "\n",
            "Train Epoch: 36 [51200/60000 (85%)]\tLoss: 17.481621\n",
            "Train Epoch: 36 [51200/60000 (85%)]\tRecon: 16.046919\n",
            "Train Epoch: 36 [51200/60000 (85%)]\tKL: 1.434701\n",
            "\n",
            "====> Epoch: 36 Average loss: 16.3722\n",
            "====> Test set loss: 124.7746\n",
            "Train Epoch: 37 [0/60000 (0%)]\tLoss: 20.102522\n",
            "Train Epoch: 37 [0/60000 (0%)]\tRecon: 18.790785\n",
            "Train Epoch: 37 [0/60000 (0%)]\tKL: 1.311738\n",
            "\n",
            "Train Epoch: 37 [12800/60000 (21%)]\tLoss: 20.563414\n",
            "Train Epoch: 37 [12800/60000 (21%)]\tRecon: 19.301594\n",
            "Train Epoch: 37 [12800/60000 (21%)]\tKL: 1.261820\n",
            "\n",
            "Train Epoch: 37 [25600/60000 (43%)]\tLoss: 21.784513\n",
            "Train Epoch: 37 [25600/60000 (43%)]\tRecon: 20.547707\n",
            "Train Epoch: 37 [25600/60000 (43%)]\tKL: 1.236808\n",
            "\n",
            "Train Epoch: 37 [38400/60000 (64%)]\tLoss: 15.867980\n",
            "Train Epoch: 37 [38400/60000 (64%)]\tRecon: 14.649744\n",
            "Train Epoch: 37 [38400/60000 (64%)]\tKL: 1.218236\n",
            "\n",
            "Train Epoch: 37 [51200/60000 (85%)]\tLoss: 18.395315\n",
            "Train Epoch: 37 [51200/60000 (85%)]\tRecon: 17.226877\n",
            "Train Epoch: 37 [51200/60000 (85%)]\tKL: 1.168438\n",
            "\n",
            "====> Epoch: 37 Average loss: 16.7325\n",
            "====> Test set loss: 139.1758\n",
            "Train Epoch: 38 [0/60000 (0%)]\tLoss: 22.390707\n",
            "Train Epoch: 38 [0/60000 (0%)]\tRecon: 21.227108\n",
            "Train Epoch: 38 [0/60000 (0%)]\tKL: 1.163600\n",
            "\n",
            "Train Epoch: 38 [12800/60000 (21%)]\tLoss: 17.409924\n",
            "Train Epoch: 38 [12800/60000 (21%)]\tRecon: 16.198921\n",
            "Train Epoch: 38 [12800/60000 (21%)]\tKL: 1.211002\n",
            "\n",
            "Train Epoch: 38 [25600/60000 (43%)]\tLoss: 10.411930\n",
            "Train Epoch: 38 [25600/60000 (43%)]\tRecon: 8.958521\n",
            "Train Epoch: 38 [25600/60000 (43%)]\tKL: 1.453410\n",
            "\n",
            "Train Epoch: 38 [38400/60000 (64%)]\tLoss: 18.766245\n",
            "Train Epoch: 38 [38400/60000 (64%)]\tRecon: 17.809334\n",
            "Train Epoch: 38 [38400/60000 (64%)]\tKL: 0.956910\n",
            "\n",
            "Train Epoch: 38 [51200/60000 (85%)]\tLoss: 7.233315\n",
            "Train Epoch: 38 [51200/60000 (85%)]\tRecon: 6.080424\n",
            "Train Epoch: 38 [51200/60000 (85%)]\tKL: 1.152891\n",
            "\n",
            "====> Epoch: 38 Average loss: 15.8273\n",
            "====> Test set loss: 157.0875\n",
            "Train Epoch: 39 [0/60000 (0%)]\tLoss: 25.778399\n",
            "Train Epoch: 39 [0/60000 (0%)]\tRecon: 24.715181\n",
            "Train Epoch: 39 [0/60000 (0%)]\tKL: 1.063218\n",
            "\n",
            "Train Epoch: 39 [12800/60000 (21%)]\tLoss: 9.783465\n",
            "Train Epoch: 39 [12800/60000 (21%)]\tRecon: 8.994560\n",
            "Train Epoch: 39 [12800/60000 (21%)]\tKL: 0.788905\n",
            "\n",
            "Train Epoch: 39 [25600/60000 (43%)]\tLoss: 7.998312\n",
            "Train Epoch: 39 [25600/60000 (43%)]\tRecon: 6.955228\n",
            "Train Epoch: 39 [25600/60000 (43%)]\tKL: 1.043083\n",
            "\n",
            "Train Epoch: 39 [38400/60000 (64%)]\tLoss: 14.995807\n",
            "Train Epoch: 39 [38400/60000 (64%)]\tRecon: 14.182291\n",
            "Train Epoch: 39 [38400/60000 (64%)]\tKL: 0.813516\n",
            "\n",
            "Train Epoch: 39 [51200/60000 (85%)]\tLoss: 14.051128\n",
            "Train Epoch: 39 [51200/60000 (85%)]\tRecon: 13.290071\n",
            "Train Epoch: 39 [51200/60000 (85%)]\tKL: 0.761057\n",
            "\n",
            "====> Epoch: 39 Average loss: 16.3210\n",
            "====> Test set loss: 68.2115\n",
            "Train Epoch: 40 [0/60000 (0%)]\tLoss: 11.293914\n",
            "Train Epoch: 40 [0/60000 (0%)]\tRecon: 10.529588\n",
            "Train Epoch: 40 [0/60000 (0%)]\tKL: 0.764326\n",
            "\n",
            "Train Epoch: 40 [12800/60000 (21%)]\tLoss: 23.967590\n",
            "Train Epoch: 40 [12800/60000 (21%)]\tRecon: 23.215513\n",
            "Train Epoch: 40 [12800/60000 (21%)]\tKL: 0.752078\n",
            "\n",
            "Train Epoch: 40 [25600/60000 (43%)]\tLoss: 15.342246\n",
            "Train Epoch: 40 [25600/60000 (43%)]\tRecon: 14.601219\n",
            "Train Epoch: 40 [25600/60000 (43%)]\tKL: 0.741027\n",
            "\n",
            "Train Epoch: 40 [38400/60000 (64%)]\tLoss: 11.463175\n",
            "Train Epoch: 40 [38400/60000 (64%)]\tRecon: 10.615509\n",
            "Train Epoch: 40 [38400/60000 (64%)]\tKL: 0.847665\n",
            "\n",
            "Train Epoch: 40 [51200/60000 (85%)]\tLoss: 19.403748\n",
            "Train Epoch: 40 [51200/60000 (85%)]\tRecon: 18.668530\n",
            "Train Epoch: 40 [51200/60000 (85%)]\tKL: 0.735218\n",
            "\n",
            "====> Epoch: 40 Average loss: 14.0072\n",
            "====> Test set loss: 112.4766\n",
            "Train Epoch: 41 [0/60000 (0%)]\tLoss: 19.130011\n",
            "Train Epoch: 41 [0/60000 (0%)]\tRecon: 18.437687\n",
            "Train Epoch: 41 [0/60000 (0%)]\tKL: 0.692324\n",
            "\n",
            "Train Epoch: 41 [12800/60000 (21%)]\tLoss: 15.410442\n",
            "Train Epoch: 41 [12800/60000 (21%)]\tRecon: 14.784569\n",
            "Train Epoch: 41 [12800/60000 (21%)]\tKL: 0.625874\n",
            "\n",
            "Train Epoch: 41 [25600/60000 (43%)]\tLoss: 17.228897\n",
            "Train Epoch: 41 [25600/60000 (43%)]\tRecon: 16.688290\n",
            "Train Epoch: 41 [25600/60000 (43%)]\tKL: 0.540608\n",
            "\n",
            "Train Epoch: 41 [38400/60000 (64%)]\tLoss: 24.824507\n",
            "Train Epoch: 41 [38400/60000 (64%)]\tRecon: 24.137854\n",
            "Train Epoch: 41 [38400/60000 (64%)]\tKL: 0.686653\n",
            "\n",
            "Train Epoch: 41 [51200/60000 (85%)]\tLoss: 11.518944\n",
            "Train Epoch: 41 [51200/60000 (85%)]\tRecon: 10.867711\n",
            "Train Epoch: 41 [51200/60000 (85%)]\tKL: 0.651233\n",
            "\n",
            "====> Epoch: 41 Average loss: 16.2621\n",
            "====> Test set loss: 107.9703\n",
            "Train Epoch: 42 [0/60000 (0%)]\tLoss: 18.536036\n",
            "Train Epoch: 42 [0/60000 (0%)]\tRecon: 17.941248\n",
            "Train Epoch: 42 [0/60000 (0%)]\tKL: 0.594787\n",
            "\n",
            "Train Epoch: 42 [12800/60000 (21%)]\tLoss: 15.689775\n",
            "Train Epoch: 42 [12800/60000 (21%)]\tRecon: 15.035419\n",
            "Train Epoch: 42 [12800/60000 (21%)]\tKL: 0.654356\n",
            "\n",
            "Train Epoch: 42 [25600/60000 (43%)]\tLoss: 17.572144\n",
            "Train Epoch: 42 [25600/60000 (43%)]\tRecon: 17.018381\n",
            "Train Epoch: 42 [25600/60000 (43%)]\tKL: 0.553763\n",
            "\n",
            "Train Epoch: 42 [38400/60000 (64%)]\tLoss: 41.409966\n",
            "Train Epoch: 42 [38400/60000 (64%)]\tRecon: 29.636818\n",
            "Train Epoch: 42 [38400/60000 (64%)]\tKL: 11.773148\n",
            "\n",
            "Train Epoch: 42 [51200/60000 (85%)]\tLoss: 36.424248\n",
            "Train Epoch: 42 [51200/60000 (85%)]\tRecon: 29.219412\n",
            "Train Epoch: 42 [51200/60000 (85%)]\tKL: 7.204834\n",
            "\n",
            "====> Epoch: 42 Average loss: 126.2073\n",
            "====> Test set loss: 3768.8528\n",
            "Train Epoch: 43 [0/60000 (0%)]\tLoss: 602.497925\n",
            "Train Epoch: 43 [0/60000 (0%)]\tRecon: 28.247282\n",
            "Train Epoch: 43 [0/60000 (0%)]\tKL: 574.250671\n",
            "\n",
            "Train Epoch: 43 [12800/60000 (21%)]\tLoss: 58.207188\n",
            "Train Epoch: 43 [12800/60000 (21%)]\tRecon: 28.883648\n",
            "Train Epoch: 43 [12800/60000 (21%)]\tKL: 29.323540\n",
            "\n",
            "Train Epoch: 43 [25600/60000 (43%)]\tLoss: 43.119232\n",
            "Train Epoch: 43 [25600/60000 (43%)]\tRecon: 28.044167\n",
            "Train Epoch: 43 [25600/60000 (43%)]\tKL: 15.075064\n",
            "\n",
            "Train Epoch: 43 [38400/60000 (64%)]\tLoss: 40.178249\n",
            "Train Epoch: 43 [38400/60000 (64%)]\tRecon: 28.185173\n",
            "Train Epoch: 43 [38400/60000 (64%)]\tKL: 11.993077\n",
            "\n",
            "Train Epoch: 43 [51200/60000 (85%)]\tLoss: 39.833328\n",
            "Train Epoch: 43 [51200/60000 (85%)]\tRecon: 29.190914\n",
            "Train Epoch: 43 [51200/60000 (85%)]\tKL: 10.642412\n",
            "\n",
            "====> Epoch: 43 Average loss: 75.9582\n",
            "====> Test set loss: 237.0327\n",
            "Train Epoch: 44 [0/60000 (0%)]\tLoss: 39.994293\n",
            "Train Epoch: 44 [0/60000 (0%)]\tRecon: 29.288696\n",
            "Train Epoch: 44 [0/60000 (0%)]\tKL: 10.705598\n",
            "\n",
            "Train Epoch: 44 [12800/60000 (21%)]\tLoss: 36.700211\n",
            "Train Epoch: 44 [12800/60000 (21%)]\tRecon: 28.193645\n",
            "Train Epoch: 44 [12800/60000 (21%)]\tKL: 8.506565\n",
            "\n",
            "Train Epoch: 44 [25600/60000 (43%)]\tLoss: 36.961143\n",
            "Train Epoch: 44 [25600/60000 (43%)]\tRecon: 28.939762\n",
            "Train Epoch: 44 [25600/60000 (43%)]\tKL: 8.021383\n",
            "\n",
            "Train Epoch: 44 [38400/60000 (64%)]\tLoss: 39.210724\n",
            "Train Epoch: 44 [38400/60000 (64%)]\tRecon: 29.430351\n",
            "Train Epoch: 44 [38400/60000 (64%)]\tKL: 9.780371\n",
            "\n",
            "Train Epoch: 44 [51200/60000 (85%)]\tLoss: 36.227276\n",
            "Train Epoch: 44 [51200/60000 (85%)]\tRecon: 28.902788\n",
            "Train Epoch: 44 [51200/60000 (85%)]\tKL: 7.324486\n",
            "\n",
            "====> Epoch: 44 Average loss: 38.2885\n",
            "====> Test set loss: 214.7188\n",
            "Train Epoch: 45 [0/60000 (0%)]\tLoss: 36.088554\n",
            "Train Epoch: 45 [0/60000 (0%)]\tRecon: 29.446106\n",
            "Train Epoch: 45 [0/60000 (0%)]\tKL: 6.642450\n",
            "\n",
            "Train Epoch: 45 [12800/60000 (21%)]\tLoss: 34.018753\n",
            "Train Epoch: 45 [12800/60000 (21%)]\tRecon: 28.731554\n",
            "Train Epoch: 45 [12800/60000 (21%)]\tKL: 5.287201\n",
            "\n",
            "Train Epoch: 45 [25600/60000 (43%)]\tLoss: 34.333706\n",
            "Train Epoch: 45 [25600/60000 (43%)]\tRecon: 29.638996\n",
            "Train Epoch: 45 [25600/60000 (43%)]\tKL: 4.694711\n",
            "\n",
            "Train Epoch: 45 [38400/60000 (64%)]\tLoss: 33.776775\n",
            "Train Epoch: 45 [38400/60000 (64%)]\tRecon: 29.027512\n",
            "Train Epoch: 45 [38400/60000 (64%)]\tKL: 4.749263\n",
            "\n",
            "Train Epoch: 45 [51200/60000 (85%)]\tLoss: 35.466187\n",
            "Train Epoch: 45 [51200/60000 (85%)]\tRecon: 29.310596\n",
            "Train Epoch: 45 [51200/60000 (85%)]\tKL: 6.155591\n",
            "\n",
            "====> Epoch: 45 Average loss: 35.0990\n",
            "====> Test set loss: 206.4957\n",
            "Train Epoch: 46 [0/60000 (0%)]\tLoss: 34.627270\n",
            "Train Epoch: 46 [0/60000 (0%)]\tRecon: 29.501297\n",
            "Train Epoch: 46 [0/60000 (0%)]\tKL: 5.125971\n",
            "\n",
            "Train Epoch: 46 [12800/60000 (21%)]\tLoss: 33.931068\n",
            "Train Epoch: 46 [12800/60000 (21%)]\tRecon: 29.250145\n",
            "Train Epoch: 46 [12800/60000 (21%)]\tKL: 4.680923\n",
            "\n",
            "Train Epoch: 46 [25600/60000 (43%)]\tLoss: 34.061958\n",
            "Train Epoch: 46 [25600/60000 (43%)]\tRecon: 29.028811\n",
            "Train Epoch: 46 [25600/60000 (43%)]\tKL: 5.033147\n",
            "\n",
            "Train Epoch: 46 [38400/60000 (64%)]\tLoss: 33.912354\n",
            "Train Epoch: 46 [38400/60000 (64%)]\tRecon: 28.983633\n",
            "Train Epoch: 46 [38400/60000 (64%)]\tKL: 4.928721\n",
            "\n",
            "Train Epoch: 46 [51200/60000 (85%)]\tLoss: 34.055309\n",
            "Train Epoch: 46 [51200/60000 (85%)]\tRecon: 29.865183\n",
            "Train Epoch: 46 [51200/60000 (85%)]\tKL: 4.190125\n",
            "\n",
            "====> Epoch: 46 Average loss: 33.6886\n",
            "====> Test set loss: 198.8175\n",
            "Train Epoch: 47 [0/60000 (0%)]\tLoss: 32.724468\n",
            "Train Epoch: 47 [0/60000 (0%)]\tRecon: 29.289865\n",
            "Train Epoch: 47 [0/60000 (0%)]\tKL: 3.434602\n",
            "\n",
            "Train Epoch: 47 [12800/60000 (21%)]\tLoss: 30.988226\n",
            "Train Epoch: 47 [12800/60000 (21%)]\tRecon: 28.035336\n",
            "Train Epoch: 47 [12800/60000 (21%)]\tKL: 2.952890\n",
            "\n",
            "Train Epoch: 47 [25600/60000 (43%)]\tLoss: 33.018288\n",
            "Train Epoch: 47 [25600/60000 (43%)]\tRecon: 29.203091\n",
            "Train Epoch: 47 [25600/60000 (43%)]\tKL: 3.815197\n",
            "\n",
            "Train Epoch: 47 [38400/60000 (64%)]\tLoss: 32.559189\n",
            "Train Epoch: 47 [38400/60000 (64%)]\tRecon: 29.286274\n",
            "Train Epoch: 47 [38400/60000 (64%)]\tKL: 3.272916\n",
            "\n",
            "Train Epoch: 47 [51200/60000 (85%)]\tLoss: 32.709019\n",
            "Train Epoch: 47 [51200/60000 (85%)]\tRecon: 29.565058\n",
            "Train Epoch: 47 [51200/60000 (85%)]\tKL: 3.143961\n",
            "\n",
            "====> Epoch: 47 Average loss: 32.8430\n",
            "====> Test set loss: 196.2146\n",
            "Train Epoch: 48 [0/60000 (0%)]\tLoss: 33.043045\n",
            "Train Epoch: 48 [0/60000 (0%)]\tRecon: 29.766747\n",
            "Train Epoch: 48 [0/60000 (0%)]\tKL: 3.276298\n",
            "\n",
            "Train Epoch: 48 [12800/60000 (21%)]\tLoss: 32.871807\n",
            "Train Epoch: 48 [12800/60000 (21%)]\tRecon: 29.517599\n",
            "Train Epoch: 48 [12800/60000 (21%)]\tKL: 3.354208\n",
            "\n",
            "Train Epoch: 48 [25600/60000 (43%)]\tLoss: 32.734135\n",
            "Train Epoch: 48 [25600/60000 (43%)]\tRecon: 29.216635\n",
            "Train Epoch: 48 [25600/60000 (43%)]\tKL: 3.517499\n",
            "\n",
            "Train Epoch: 48 [38400/60000 (64%)]\tLoss: 32.199841\n",
            "Train Epoch: 48 [38400/60000 (64%)]\tRecon: 29.625475\n",
            "Train Epoch: 48 [38400/60000 (64%)]\tKL: 2.574367\n",
            "\n",
            "Train Epoch: 48 [51200/60000 (85%)]\tLoss: 32.359196\n",
            "Train Epoch: 48 [51200/60000 (85%)]\tRecon: 29.755390\n",
            "Train Epoch: 48 [51200/60000 (85%)]\tKL: 2.603806\n",
            "\n",
            "====> Epoch: 48 Average loss: 32.4513\n",
            "====> Test set loss: 254.0614\n",
            "Train Epoch: 49 [0/60000 (0%)]\tLoss: 42.504490\n",
            "Train Epoch: 49 [0/60000 (0%)]\tRecon: 29.234001\n",
            "Train Epoch: 49 [0/60000 (0%)]\tKL: 13.270489\n",
            "\n",
            "Train Epoch: 49 [12800/60000 (21%)]\tLoss: 35.189148\n",
            "Train Epoch: 49 [12800/60000 (21%)]\tRecon: 29.523823\n",
            "Train Epoch: 49 [12800/60000 (21%)]\tKL: 5.665325\n",
            "\n",
            "Train Epoch: 49 [25600/60000 (43%)]\tLoss: 32.972252\n",
            "Train Epoch: 49 [25600/60000 (43%)]\tRecon: 28.710575\n",
            "Train Epoch: 49 [25600/60000 (43%)]\tKL: 4.261676\n",
            "\n",
            "Train Epoch: 49 [38400/60000 (64%)]\tLoss: 32.048061\n",
            "Train Epoch: 49 [38400/60000 (64%)]\tRecon: 29.245625\n",
            "Train Epoch: 49 [38400/60000 (64%)]\tKL: 2.802437\n",
            "\n",
            "Train Epoch: 49 [51200/60000 (85%)]\tLoss: 31.297674\n",
            "Train Epoch: 49 [51200/60000 (85%)]\tRecon: 28.872551\n",
            "Train Epoch: 49 [51200/60000 (85%)]\tKL: 2.425123\n",
            "\n",
            "====> Epoch: 49 Average loss: 33.7461\n",
            "====> Test set loss: 190.0808\n",
            "Train Epoch: 50 [0/60000 (0%)]\tLoss: 32.144039\n",
            "Train Epoch: 50 [0/60000 (0%)]\tRecon: 29.581844\n",
            "Train Epoch: 50 [0/60000 (0%)]\tKL: 2.562195\n",
            "\n",
            "Train Epoch: 50 [12800/60000 (21%)]\tLoss: 31.790028\n",
            "Train Epoch: 50 [12800/60000 (21%)]\tRecon: 29.552341\n",
            "Train Epoch: 50 [12800/60000 (21%)]\tKL: 2.237686\n",
            "\n",
            "Train Epoch: 50 [25600/60000 (43%)]\tLoss: 30.967134\n",
            "Train Epoch: 50 [25600/60000 (43%)]\tRecon: 28.775581\n",
            "Train Epoch: 50 [25600/60000 (43%)]\tKL: 2.191552\n",
            "\n",
            "Train Epoch: 50 [38400/60000 (64%)]\tLoss: 31.564514\n",
            "Train Epoch: 50 [38400/60000 (64%)]\tRecon: 29.448277\n",
            "Train Epoch: 50 [38400/60000 (64%)]\tKL: 2.116237\n",
            "\n",
            "Train Epoch: 50 [51200/60000 (85%)]\tLoss: 30.863958\n",
            "Train Epoch: 50 [51200/60000 (85%)]\tRecon: 29.052864\n",
            "Train Epoch: 50 [51200/60000 (85%)]\tKL: 1.811094\n",
            "\n",
            "====> Epoch: 50 Average loss: 31.4611\n",
            "====> Test set loss: 187.3112\n",
            "Train Epoch: 51 [0/60000 (0%)]\tLoss: 31.369762\n",
            "Train Epoch: 51 [0/60000 (0%)]\tRecon: 29.585182\n",
            "Train Epoch: 51 [0/60000 (0%)]\tKL: 1.784580\n",
            "\n",
            "Train Epoch: 51 [12800/60000 (21%)]\tLoss: 30.867689\n",
            "Train Epoch: 51 [12800/60000 (21%)]\tRecon: 29.168533\n",
            "Train Epoch: 51 [12800/60000 (21%)]\tKL: 1.699156\n",
            "\n",
            "Train Epoch: 51 [25600/60000 (43%)]\tLoss: 31.169138\n",
            "Train Epoch: 51 [25600/60000 (43%)]\tRecon: 29.442051\n",
            "Train Epoch: 51 [25600/60000 (43%)]\tKL: 1.727086\n",
            "\n",
            "Train Epoch: 51 [38400/60000 (64%)]\tLoss: 30.931871\n",
            "Train Epoch: 51 [38400/60000 (64%)]\tRecon: 29.036863\n",
            "Train Epoch: 51 [38400/60000 (64%)]\tKL: 1.895007\n",
            "\n",
            "Train Epoch: 51 [51200/60000 (85%)]\tLoss: 31.391218\n",
            "Train Epoch: 51 [51200/60000 (85%)]\tRecon: 29.619104\n",
            "Train Epoch: 51 [51200/60000 (85%)]\tKL: 1.772114\n",
            "\n",
            "====> Epoch: 51 Average loss: 31.0529\n",
            "====> Test set loss: 184.8869\n",
            "Train Epoch: 52 [0/60000 (0%)]\tLoss: 31.180191\n",
            "Train Epoch: 52 [0/60000 (0%)]\tRecon: 29.488426\n",
            "Train Epoch: 52 [0/60000 (0%)]\tKL: 1.691765\n",
            "\n",
            "Train Epoch: 52 [12800/60000 (21%)]\tLoss: 31.881617\n",
            "Train Epoch: 52 [12800/60000 (21%)]\tRecon: 29.006958\n",
            "Train Epoch: 52 [12800/60000 (21%)]\tKL: 2.874659\n",
            "\n",
            "Train Epoch: 52 [25600/60000 (43%)]\tLoss: 31.803324\n",
            "Train Epoch: 52 [25600/60000 (43%)]\tRecon: 30.000217\n",
            "Train Epoch: 52 [25600/60000 (43%)]\tKL: 1.803106\n",
            "\n",
            "Train Epoch: 52 [38400/60000 (64%)]\tLoss: 30.312540\n",
            "Train Epoch: 52 [38400/60000 (64%)]\tRecon: 28.763332\n",
            "Train Epoch: 52 [38400/60000 (64%)]\tKL: 1.549208\n",
            "\n",
            "Train Epoch: 52 [51200/60000 (85%)]\tLoss: 30.597691\n",
            "Train Epoch: 52 [51200/60000 (85%)]\tRecon: 29.247805\n",
            "Train Epoch: 52 [51200/60000 (85%)]\tKL: 1.349886\n",
            "\n",
            "====> Epoch: 52 Average loss: 30.8420\n",
            "====> Test set loss: 184.7203\n",
            "Train Epoch: 53 [0/60000 (0%)]\tLoss: 30.751265\n",
            "Train Epoch: 53 [0/60000 (0%)]\tRecon: 29.191729\n",
            "Train Epoch: 53 [0/60000 (0%)]\tKL: 1.559535\n",
            "\n",
            "Train Epoch: 53 [12800/60000 (21%)]\tLoss: 30.393143\n",
            "Train Epoch: 53 [12800/60000 (21%)]\tRecon: 29.035898\n",
            "Train Epoch: 53 [12800/60000 (21%)]\tKL: 1.357245\n",
            "\n",
            "Train Epoch: 53 [25600/60000 (43%)]\tLoss: 30.522369\n",
            "Train Epoch: 53 [25600/60000 (43%)]\tRecon: 29.103489\n",
            "Train Epoch: 53 [25600/60000 (43%)]\tKL: 1.418880\n",
            "\n",
            "Train Epoch: 53 [38400/60000 (64%)]\tLoss: 30.399670\n",
            "Train Epoch: 53 [38400/60000 (64%)]\tRecon: 29.016071\n",
            "Train Epoch: 53 [38400/60000 (64%)]\tKL: 1.383598\n",
            "\n",
            "Train Epoch: 53 [51200/60000 (85%)]\tLoss: 30.657167\n",
            "Train Epoch: 53 [51200/60000 (85%)]\tRecon: 29.241026\n",
            "Train Epoch: 53 [51200/60000 (85%)]\tKL: 1.416141\n",
            "\n",
            "====> Epoch: 53 Average loss: 30.6469\n",
            "====> Test set loss: 183.4006\n",
            "Train Epoch: 54 [0/60000 (0%)]\tLoss: 30.214149\n",
            "Train Epoch: 54 [0/60000 (0%)]\tRecon: 28.970663\n",
            "Train Epoch: 54 [0/60000 (0%)]\tKL: 1.243487\n",
            "\n",
            "Train Epoch: 54 [12800/60000 (21%)]\tLoss: 30.000160\n",
            "Train Epoch: 54 [12800/60000 (21%)]\tRecon: 28.772144\n",
            "Train Epoch: 54 [12800/60000 (21%)]\tKL: 1.228016\n",
            "\n",
            "Train Epoch: 54 [25600/60000 (43%)]\tLoss: 29.652489\n",
            "Train Epoch: 54 [25600/60000 (43%)]\tRecon: 28.502760\n",
            "Train Epoch: 54 [25600/60000 (43%)]\tKL: 1.149730\n",
            "\n",
            "Train Epoch: 54 [38400/60000 (64%)]\tLoss: 29.432550\n",
            "Train Epoch: 54 [38400/60000 (64%)]\tRecon: 28.374949\n",
            "Train Epoch: 54 [38400/60000 (64%)]\tKL: 1.057602\n",
            "\n",
            "Train Epoch: 54 [51200/60000 (85%)]\tLoss: 30.237885\n",
            "Train Epoch: 54 [51200/60000 (85%)]\tRecon: 28.995979\n",
            "Train Epoch: 54 [51200/60000 (85%)]\tKL: 1.241904\n",
            "\n",
            "====> Epoch: 54 Average loss: 30.5406\n",
            "====> Test set loss: 182.3928\n",
            "Train Epoch: 55 [0/60000 (0%)]\tLoss: 30.207270\n",
            "Train Epoch: 55 [0/60000 (0%)]\tRecon: 29.014496\n",
            "Train Epoch: 55 [0/60000 (0%)]\tKL: 1.192774\n",
            "\n",
            "Train Epoch: 55 [12800/60000 (21%)]\tLoss: 30.943155\n",
            "Train Epoch: 55 [12800/60000 (21%)]\tRecon: 29.867371\n",
            "Train Epoch: 55 [12800/60000 (21%)]\tKL: 1.075784\n",
            "\n",
            "Train Epoch: 55 [25600/60000 (43%)]\tLoss: 29.884243\n",
            "Train Epoch: 55 [25600/60000 (43%)]\tRecon: 28.670219\n",
            "Train Epoch: 55 [25600/60000 (43%)]\tKL: 1.214023\n",
            "\n",
            "Train Epoch: 55 [38400/60000 (64%)]\tLoss: 29.630116\n",
            "Train Epoch: 55 [38400/60000 (64%)]\tRecon: 28.614983\n",
            "Train Epoch: 55 [38400/60000 (64%)]\tKL: 1.015134\n",
            "\n",
            "Train Epoch: 55 [51200/60000 (85%)]\tLoss: 29.966724\n",
            "Train Epoch: 55 [51200/60000 (85%)]\tRecon: 28.973156\n",
            "Train Epoch: 55 [51200/60000 (85%)]\tKL: 0.993569\n",
            "\n",
            "====> Epoch: 55 Average loss: 30.3587\n",
            "====> Test set loss: 181.8001\n",
            "Train Epoch: 56 [0/60000 (0%)]\tLoss: 30.821165\n",
            "Train Epoch: 56 [0/60000 (0%)]\tRecon: 29.614964\n",
            "Train Epoch: 56 [0/60000 (0%)]\tKL: 1.206201\n",
            "\n",
            "Train Epoch: 56 [12800/60000 (21%)]\tLoss: 30.576796\n",
            "Train Epoch: 56 [12800/60000 (21%)]\tRecon: 29.541096\n",
            "Train Epoch: 56 [12800/60000 (21%)]\tKL: 1.035700\n",
            "\n",
            "Train Epoch: 56 [25600/60000 (43%)]\tLoss: 29.704535\n",
            "Train Epoch: 56 [25600/60000 (43%)]\tRecon: 28.801533\n",
            "Train Epoch: 56 [25600/60000 (43%)]\tKL: 0.903001\n",
            "\n",
            "Train Epoch: 56 [38400/60000 (64%)]\tLoss: 30.606592\n",
            "Train Epoch: 56 [38400/60000 (64%)]\tRecon: 29.511871\n",
            "Train Epoch: 56 [38400/60000 (64%)]\tKL: 1.094721\n",
            "\n",
            "Train Epoch: 56 [51200/60000 (85%)]\tLoss: 30.535856\n",
            "Train Epoch: 56 [51200/60000 (85%)]\tRecon: 29.460003\n",
            "Train Epoch: 56 [51200/60000 (85%)]\tKL: 1.075854\n",
            "\n",
            "====> Epoch: 56 Average loss: 30.2560\n",
            "====> Test set loss: 180.4782\n",
            "Train Epoch: 57 [0/60000 (0%)]\tLoss: 29.814445\n",
            "Train Epoch: 57 [0/60000 (0%)]\tRecon: 28.680935\n",
            "Train Epoch: 57 [0/60000 (0%)]\tKL: 1.133510\n",
            "\n",
            "Train Epoch: 57 [12800/60000 (21%)]\tLoss: 30.537724\n",
            "Train Epoch: 57 [12800/60000 (21%)]\tRecon: 29.443554\n",
            "Train Epoch: 57 [12800/60000 (21%)]\tKL: 1.094170\n",
            "\n",
            "Train Epoch: 57 [25600/60000 (43%)]\tLoss: 30.413521\n",
            "Train Epoch: 57 [25600/60000 (43%)]\tRecon: 29.398607\n",
            "Train Epoch: 57 [25600/60000 (43%)]\tKL: 1.014913\n",
            "\n",
            "Train Epoch: 57 [38400/60000 (64%)]\tLoss: 29.050867\n",
            "Train Epoch: 57 [38400/60000 (64%)]\tRecon: 28.098330\n",
            "Train Epoch: 57 [38400/60000 (64%)]\tKL: 0.952538\n",
            "\n",
            "Train Epoch: 57 [51200/60000 (85%)]\tLoss: 30.444143\n",
            "Train Epoch: 57 [51200/60000 (85%)]\tRecon: 29.491726\n",
            "Train Epoch: 57 [51200/60000 (85%)]\tKL: 0.952418\n",
            "\n",
            "====> Epoch: 57 Average loss: 30.2140\n",
            "====> Test set loss: 180.6714\n",
            "Train Epoch: 58 [0/60000 (0%)]\tLoss: 30.184368\n",
            "Train Epoch: 58 [0/60000 (0%)]\tRecon: 29.281250\n",
            "Train Epoch: 58 [0/60000 (0%)]\tKL: 0.903118\n",
            "\n",
            "Train Epoch: 58 [12800/60000 (21%)]\tLoss: 29.332333\n",
            "Train Epoch: 58 [12800/60000 (21%)]\tRecon: 28.495007\n",
            "Train Epoch: 58 [12800/60000 (21%)]\tKL: 0.837327\n",
            "\n",
            "Train Epoch: 58 [25600/60000 (43%)]\tLoss: 29.570333\n",
            "Train Epoch: 58 [25600/60000 (43%)]\tRecon: 28.702675\n",
            "Train Epoch: 58 [25600/60000 (43%)]\tKL: 0.867658\n",
            "\n",
            "Train Epoch: 58 [38400/60000 (64%)]\tLoss: 29.577740\n",
            "Train Epoch: 58 [38400/60000 (64%)]\tRecon: 28.804232\n",
            "Train Epoch: 58 [38400/60000 (64%)]\tKL: 0.773507\n",
            "\n",
            "Train Epoch: 58 [51200/60000 (85%)]\tLoss: 29.774172\n",
            "Train Epoch: 58 [51200/60000 (85%)]\tRecon: 28.869745\n",
            "Train Epoch: 58 [51200/60000 (85%)]\tKL: 0.904426\n",
            "\n",
            "====> Epoch: 58 Average loss: 30.1739\n",
            "====> Test set loss: 181.4279\n",
            "Train Epoch: 59 [0/60000 (0%)]\tLoss: 31.620781\n",
            "Train Epoch: 59 [0/60000 (0%)]\tRecon: 30.713369\n",
            "Train Epoch: 59 [0/60000 (0%)]\tKL: 0.907411\n",
            "\n",
            "Train Epoch: 59 [12800/60000 (21%)]\tLoss: 30.436525\n",
            "Train Epoch: 59 [12800/60000 (21%)]\tRecon: 29.551605\n",
            "Train Epoch: 59 [12800/60000 (21%)]\tKL: 0.884920\n",
            "\n",
            "Train Epoch: 59 [25600/60000 (43%)]\tLoss: 30.092607\n",
            "Train Epoch: 59 [25600/60000 (43%)]\tRecon: 28.984392\n",
            "Train Epoch: 59 [25600/60000 (43%)]\tKL: 1.108215\n",
            "\n",
            "Train Epoch: 59 [38400/60000 (64%)]\tLoss: 30.351954\n",
            "Train Epoch: 59 [38400/60000 (64%)]\tRecon: 29.474207\n",
            "Train Epoch: 59 [38400/60000 (64%)]\tKL: 0.877746\n",
            "\n",
            "Train Epoch: 59 [51200/60000 (85%)]\tLoss: 30.242447\n",
            "Train Epoch: 59 [51200/60000 (85%)]\tRecon: 29.275501\n",
            "Train Epoch: 59 [51200/60000 (85%)]\tKL: 0.966946\n",
            "\n",
            "====> Epoch: 59 Average loss: 30.2491\n",
            "====> Test set loss: 179.6232\n",
            "Train Epoch: 60 [0/60000 (0%)]\tLoss: 30.373163\n",
            "Train Epoch: 60 [0/60000 (0%)]\tRecon: 29.628569\n",
            "Train Epoch: 60 [0/60000 (0%)]\tKL: 0.744595\n",
            "\n",
            "Train Epoch: 60 [12800/60000 (21%)]\tLoss: 29.949022\n",
            "Train Epoch: 60 [12800/60000 (21%)]\tRecon: 29.090046\n",
            "Train Epoch: 60 [12800/60000 (21%)]\tKL: 0.858976\n",
            "\n",
            "Train Epoch: 60 [25600/60000 (43%)]\tLoss: 28.917686\n",
            "Train Epoch: 60 [25600/60000 (43%)]\tRecon: 28.162409\n",
            "Train Epoch: 60 [25600/60000 (43%)]\tKL: 0.755277\n",
            "\n",
            "Train Epoch: 60 [38400/60000 (64%)]\tLoss: 29.542025\n",
            "Train Epoch: 60 [38400/60000 (64%)]\tRecon: 28.735140\n",
            "Train Epoch: 60 [38400/60000 (64%)]\tKL: 0.806885\n",
            "\n",
            "Train Epoch: 60 [51200/60000 (85%)]\tLoss: 30.282532\n",
            "Train Epoch: 60 [51200/60000 (85%)]\tRecon: 29.542408\n",
            "Train Epoch: 60 [51200/60000 (85%)]\tKL: 0.740124\n",
            "\n",
            "====> Epoch: 60 Average loss: 30.0947\n",
            "====> Test set loss: 180.7685\n",
            "Train Epoch: 61 [0/60000 (0%)]\tLoss: 29.666422\n",
            "Train Epoch: 61 [0/60000 (0%)]\tRecon: 29.017864\n",
            "Train Epoch: 61 [0/60000 (0%)]\tKL: 0.648557\n",
            "\n",
            "Train Epoch: 61 [12800/60000 (21%)]\tLoss: 30.454979\n",
            "Train Epoch: 61 [12800/60000 (21%)]\tRecon: 29.692108\n",
            "Train Epoch: 61 [12800/60000 (21%)]\tKL: 0.762870\n",
            "\n",
            "Train Epoch: 61 [25600/60000 (43%)]\tLoss: 29.501068\n",
            "Train Epoch: 61 [25600/60000 (43%)]\tRecon: 28.566910\n",
            "Train Epoch: 61 [25600/60000 (43%)]\tKL: 0.934159\n",
            "\n",
            "Train Epoch: 61 [38400/60000 (64%)]\tLoss: 30.318134\n",
            "Train Epoch: 61 [38400/60000 (64%)]\tRecon: 29.472515\n",
            "Train Epoch: 61 [38400/60000 (64%)]\tKL: 0.845619\n",
            "\n",
            "Train Epoch: 61 [51200/60000 (85%)]\tLoss: 29.848652\n",
            "Train Epoch: 61 [51200/60000 (85%)]\tRecon: 29.112782\n",
            "Train Epoch: 61 [51200/60000 (85%)]\tKL: 0.735870\n",
            "\n",
            "====> Epoch: 61 Average loss: 30.1330\n",
            "====> Test set loss: 180.5290\n",
            "Train Epoch: 62 [0/60000 (0%)]\tLoss: 30.537086\n",
            "Train Epoch: 62 [0/60000 (0%)]\tRecon: 29.790092\n",
            "Train Epoch: 62 [0/60000 (0%)]\tKL: 0.746995\n",
            "\n",
            "Train Epoch: 62 [12800/60000 (21%)]\tLoss: 30.361734\n",
            "Train Epoch: 62 [12800/60000 (21%)]\tRecon: 29.603304\n",
            "Train Epoch: 62 [12800/60000 (21%)]\tKL: 0.758431\n",
            "\n",
            "Train Epoch: 62 [25600/60000 (43%)]\tLoss: 29.878014\n",
            "Train Epoch: 62 [25600/60000 (43%)]\tRecon: 29.178761\n",
            "Train Epoch: 62 [25600/60000 (43%)]\tKL: 0.699253\n",
            "\n",
            "Train Epoch: 62 [38400/60000 (64%)]\tLoss: 30.204334\n",
            "Train Epoch: 62 [38400/60000 (64%)]\tRecon: 29.286268\n",
            "Train Epoch: 62 [38400/60000 (64%)]\tKL: 0.918066\n",
            "\n",
            "Train Epoch: 62 [51200/60000 (85%)]\tLoss: 29.684330\n",
            "Train Epoch: 62 [51200/60000 (85%)]\tRecon: 28.848595\n",
            "Train Epoch: 62 [51200/60000 (85%)]\tKL: 0.835735\n",
            "\n",
            "====> Epoch: 62 Average loss: 30.1387\n",
            "====> Test set loss: 181.3892\n",
            "Train Epoch: 63 [0/60000 (0%)]\tLoss: 29.460052\n",
            "Train Epoch: 63 [0/60000 (0%)]\tRecon: 28.420444\n",
            "Train Epoch: 63 [0/60000 (0%)]\tKL: 1.039607\n",
            "\n",
            "Train Epoch: 63 [12800/60000 (21%)]\tLoss: 31.004477\n",
            "Train Epoch: 63 [12800/60000 (21%)]\tRecon: 29.697559\n",
            "Train Epoch: 63 [12800/60000 (21%)]\tKL: 1.306917\n",
            "\n",
            "Train Epoch: 63 [25600/60000 (43%)]\tLoss: 30.480585\n",
            "Train Epoch: 63 [25600/60000 (43%)]\tRecon: 29.524166\n",
            "Train Epoch: 63 [25600/60000 (43%)]\tKL: 0.956419\n",
            "\n",
            "Train Epoch: 63 [38400/60000 (64%)]\tLoss: 29.809868\n",
            "Train Epoch: 63 [38400/60000 (64%)]\tRecon: 28.922920\n",
            "Train Epoch: 63 [38400/60000 (64%)]\tKL: 0.886948\n",
            "\n",
            "Train Epoch: 63 [51200/60000 (85%)]\tLoss: 30.110777\n",
            "Train Epoch: 63 [51200/60000 (85%)]\tRecon: 29.167259\n",
            "Train Epoch: 63 [51200/60000 (85%)]\tKL: 0.943517\n",
            "\n",
            "====> Epoch: 63 Average loss: 30.1019\n",
            "====> Test set loss: 178.8438\n",
            "Train Epoch: 64 [0/60000 (0%)]\tLoss: 29.764433\n",
            "Train Epoch: 64 [0/60000 (0%)]\tRecon: 29.020542\n",
            "Train Epoch: 64 [0/60000 (0%)]\tKL: 0.743890\n",
            "\n",
            "Train Epoch: 64 [12800/60000 (21%)]\tLoss: 30.195711\n",
            "Train Epoch: 64 [12800/60000 (21%)]\tRecon: 29.376141\n",
            "Train Epoch: 64 [12800/60000 (21%)]\tKL: 0.819571\n",
            "\n",
            "Train Epoch: 64 [25600/60000 (43%)]\tLoss: 30.003922\n",
            "Train Epoch: 64 [25600/60000 (43%)]\tRecon: 29.180458\n",
            "Train Epoch: 64 [25600/60000 (43%)]\tKL: 0.823464\n",
            "\n",
            "Train Epoch: 64 [38400/60000 (64%)]\tLoss: 29.888557\n",
            "Train Epoch: 64 [38400/60000 (64%)]\tRecon: 29.157396\n",
            "Train Epoch: 64 [38400/60000 (64%)]\tKL: 0.731160\n",
            "\n",
            "Train Epoch: 64 [51200/60000 (85%)]\tLoss: 30.094563\n",
            "Train Epoch: 64 [51200/60000 (85%)]\tRecon: 29.222759\n",
            "Train Epoch: 64 [51200/60000 (85%)]\tKL: 0.871803\n",
            "\n",
            "====> Epoch: 64 Average loss: 29.9176\n",
            "====> Test set loss: 178.9699\n",
            "Train Epoch: 65 [0/60000 (0%)]\tLoss: 30.605690\n",
            "Train Epoch: 65 [0/60000 (0%)]\tRecon: 29.968836\n",
            "Train Epoch: 65 [0/60000 (0%)]\tKL: 0.636854\n",
            "\n",
            "Train Epoch: 65 [12800/60000 (21%)]\tLoss: 28.779736\n",
            "Train Epoch: 65 [12800/60000 (21%)]\tRecon: 28.100542\n",
            "Train Epoch: 65 [12800/60000 (21%)]\tKL: 0.679193\n",
            "\n",
            "Train Epoch: 65 [25600/60000 (43%)]\tLoss: 30.500313\n",
            "Train Epoch: 65 [25600/60000 (43%)]\tRecon: 29.505787\n",
            "Train Epoch: 65 [25600/60000 (43%)]\tKL: 0.994526\n",
            "\n",
            "Train Epoch: 65 [38400/60000 (64%)]\tLoss: 29.584261\n",
            "Train Epoch: 65 [38400/60000 (64%)]\tRecon: 28.425850\n",
            "Train Epoch: 65 [38400/60000 (64%)]\tKL: 1.158411\n",
            "\n",
            "Train Epoch: 65 [51200/60000 (85%)]\tLoss: 30.274828\n",
            "Train Epoch: 65 [51200/60000 (85%)]\tRecon: 29.350744\n",
            "Train Epoch: 65 [51200/60000 (85%)]\tKL: 0.924084\n",
            "\n",
            "====> Epoch: 65 Average loss: 30.0224\n",
            "====> Test set loss: 180.0409\n",
            "Train Epoch: 66 [0/60000 (0%)]\tLoss: 29.832005\n",
            "Train Epoch: 66 [0/60000 (0%)]\tRecon: 29.151325\n",
            "Train Epoch: 66 [0/60000 (0%)]\tKL: 0.680680\n",
            "\n",
            "Train Epoch: 66 [12800/60000 (21%)]\tLoss: 30.630880\n",
            "Train Epoch: 66 [12800/60000 (21%)]\tRecon: 29.460730\n",
            "Train Epoch: 66 [12800/60000 (21%)]\tKL: 1.170150\n",
            "\n",
            "Train Epoch: 66 [25600/60000 (43%)]\tLoss: 29.897446\n",
            "Train Epoch: 66 [25600/60000 (43%)]\tRecon: 29.233135\n",
            "Train Epoch: 66 [25600/60000 (43%)]\tKL: 0.664311\n",
            "\n",
            "Train Epoch: 66 [38400/60000 (64%)]\tLoss: 29.350559\n",
            "Train Epoch: 66 [38400/60000 (64%)]\tRecon: 28.771952\n",
            "Train Epoch: 66 [38400/60000 (64%)]\tKL: 0.578608\n",
            "\n",
            "Train Epoch: 66 [51200/60000 (85%)]\tLoss: 30.328413\n",
            "Train Epoch: 66 [51200/60000 (85%)]\tRecon: 29.719961\n",
            "Train Epoch: 66 [51200/60000 (85%)]\tKL: 0.608452\n",
            "\n",
            "====> Epoch: 66 Average loss: 29.9559\n",
            "====> Test set loss: 179.6169\n",
            "Train Epoch: 67 [0/60000 (0%)]\tLoss: 29.700863\n",
            "Train Epoch: 67 [0/60000 (0%)]\tRecon: 29.096506\n",
            "Train Epoch: 67 [0/60000 (0%)]\tKL: 0.604357\n",
            "\n",
            "Train Epoch: 67 [12800/60000 (21%)]\tLoss: 29.455751\n",
            "Train Epoch: 67 [12800/60000 (21%)]\tRecon: 28.876293\n",
            "Train Epoch: 67 [12800/60000 (21%)]\tKL: 0.579459\n",
            "\n",
            "Train Epoch: 67 [25600/60000 (43%)]\tLoss: 29.648643\n",
            "Train Epoch: 67 [25600/60000 (43%)]\tRecon: 28.916265\n",
            "Train Epoch: 67 [25600/60000 (43%)]\tKL: 0.732379\n",
            "\n",
            "Train Epoch: 67 [38400/60000 (64%)]\tLoss: 30.348362\n",
            "Train Epoch: 67 [38400/60000 (64%)]\tRecon: 29.613234\n",
            "Train Epoch: 67 [38400/60000 (64%)]\tKL: 0.735128\n",
            "\n",
            "Train Epoch: 67 [51200/60000 (85%)]\tLoss: 29.921637\n",
            "Train Epoch: 67 [51200/60000 (85%)]\tRecon: 29.217529\n",
            "Train Epoch: 67 [51200/60000 (85%)]\tKL: 0.704107\n",
            "\n",
            "====> Epoch: 67 Average loss: 29.8742\n",
            "====> Test set loss: 178.8972\n",
            "Train Epoch: 68 [0/60000 (0%)]\tLoss: 29.111132\n",
            "Train Epoch: 68 [0/60000 (0%)]\tRecon: 28.520588\n",
            "Train Epoch: 68 [0/60000 (0%)]\tKL: 0.590543\n",
            "\n",
            "Train Epoch: 68 [12800/60000 (21%)]\tLoss: 29.534924\n",
            "Train Epoch: 68 [12800/60000 (21%)]\tRecon: 28.844990\n",
            "Train Epoch: 68 [12800/60000 (21%)]\tKL: 0.689934\n",
            "\n",
            "Train Epoch: 68 [25600/60000 (43%)]\tLoss: 29.734112\n",
            "Train Epoch: 68 [25600/60000 (43%)]\tRecon: 28.951813\n",
            "Train Epoch: 68 [25600/60000 (43%)]\tKL: 0.782298\n",
            "\n",
            "Train Epoch: 68 [38400/60000 (64%)]\tLoss: 30.074266\n",
            "Train Epoch: 68 [38400/60000 (64%)]\tRecon: 29.153316\n",
            "Train Epoch: 68 [38400/60000 (64%)]\tKL: 0.920950\n",
            "\n",
            "Train Epoch: 68 [51200/60000 (85%)]\tLoss: 30.228863\n",
            "Train Epoch: 68 [51200/60000 (85%)]\tRecon: 29.267164\n",
            "Train Epoch: 68 [51200/60000 (85%)]\tKL: 0.961699\n",
            "\n",
            "====> Epoch: 68 Average loss: 29.9302\n",
            "====> Test set loss: 180.3083\n",
            "Train Epoch: 69 [0/60000 (0%)]\tLoss: 30.279591\n",
            "Train Epoch: 69 [0/60000 (0%)]\tRecon: 29.529007\n",
            "Train Epoch: 69 [0/60000 (0%)]\tKL: 0.750584\n",
            "\n",
            "Train Epoch: 69 [12800/60000 (21%)]\tLoss: 29.558336\n",
            "Train Epoch: 69 [12800/60000 (21%)]\tRecon: 28.762262\n",
            "Train Epoch: 69 [12800/60000 (21%)]\tKL: 0.796074\n",
            "\n",
            "Train Epoch: 69 [25600/60000 (43%)]\tLoss: 29.762737\n",
            "Train Epoch: 69 [25600/60000 (43%)]\tRecon: 28.857151\n",
            "Train Epoch: 69 [25600/60000 (43%)]\tKL: 0.905586\n",
            "\n",
            "Train Epoch: 69 [38400/60000 (64%)]\tLoss: 29.905334\n",
            "Train Epoch: 69 [38400/60000 (64%)]\tRecon: 29.005106\n",
            "Train Epoch: 69 [38400/60000 (64%)]\tKL: 0.900229\n",
            "\n",
            "Train Epoch: 69 [51200/60000 (85%)]\tLoss: 30.413746\n",
            "Train Epoch: 69 [51200/60000 (85%)]\tRecon: 29.647806\n",
            "Train Epoch: 69 [51200/60000 (85%)]\tKL: 0.765940\n",
            "\n",
            "====> Epoch: 69 Average loss: 30.0256\n",
            "====> Test set loss: 179.6009\n",
            "Train Epoch: 70 [0/60000 (0%)]\tLoss: 30.227747\n",
            "Train Epoch: 70 [0/60000 (0%)]\tRecon: 29.479702\n",
            "Train Epoch: 70 [0/60000 (0%)]\tKL: 0.748045\n",
            "\n",
            "Train Epoch: 70 [12800/60000 (21%)]\tLoss: 31.380117\n",
            "Train Epoch: 70 [12800/60000 (21%)]\tRecon: 30.300482\n",
            "Train Epoch: 70 [12800/60000 (21%)]\tKL: 1.079635\n",
            "\n",
            "Train Epoch: 70 [25600/60000 (43%)]\tLoss: 30.730213\n",
            "Train Epoch: 70 [25600/60000 (43%)]\tRecon: 29.498989\n",
            "Train Epoch: 70 [25600/60000 (43%)]\tKL: 1.231225\n",
            "\n",
            "Train Epoch: 70 [38400/60000 (64%)]\tLoss: 30.127243\n",
            "Train Epoch: 70 [38400/60000 (64%)]\tRecon: 29.164818\n",
            "Train Epoch: 70 [38400/60000 (64%)]\tKL: 0.962426\n",
            "\n",
            "Train Epoch: 70 [51200/60000 (85%)]\tLoss: 29.605154\n",
            "Train Epoch: 70 [51200/60000 (85%)]\tRecon: 29.158222\n",
            "Train Epoch: 70 [51200/60000 (85%)]\tKL: 0.446932\n",
            "\n",
            "====> Epoch: 70 Average loss: 29.9973\n",
            "====> Test set loss: 180.1685\n",
            "Train Epoch: 71 [0/60000 (0%)]\tLoss: 29.918158\n",
            "Train Epoch: 71 [0/60000 (0%)]\tRecon: 29.312180\n",
            "Train Epoch: 71 [0/60000 (0%)]\tKL: 0.605979\n",
            "\n",
            "Train Epoch: 71 [12800/60000 (21%)]\tLoss: 30.115486\n",
            "Train Epoch: 71 [12800/60000 (21%)]\tRecon: 29.400154\n",
            "Train Epoch: 71 [12800/60000 (21%)]\tKL: 0.715332\n",
            "\n",
            "Train Epoch: 71 [25600/60000 (43%)]\tLoss: 29.598841\n",
            "Train Epoch: 71 [25600/60000 (43%)]\tRecon: 28.930490\n",
            "Train Epoch: 71 [25600/60000 (43%)]\tKL: 0.668350\n",
            "\n",
            "Train Epoch: 71 [38400/60000 (64%)]\tLoss: 29.211191\n",
            "Train Epoch: 71 [38400/60000 (64%)]\tRecon: 28.642086\n",
            "Train Epoch: 71 [38400/60000 (64%)]\tKL: 0.569105\n",
            "\n",
            "Train Epoch: 71 [51200/60000 (85%)]\tLoss: 29.463474\n",
            "Train Epoch: 71 [51200/60000 (85%)]\tRecon: 28.838326\n",
            "Train Epoch: 71 [51200/60000 (85%)]\tKL: 0.625148\n",
            "\n",
            "====> Epoch: 71 Average loss: 29.8443\n",
            "====> Test set loss: 178.0403\n",
            "Train Epoch: 72 [0/60000 (0%)]\tLoss: 30.193848\n",
            "Train Epoch: 72 [0/60000 (0%)]\tRecon: 29.578955\n",
            "Train Epoch: 72 [0/60000 (0%)]\tKL: 0.614893\n",
            "\n",
            "Train Epoch: 72 [12800/60000 (21%)]\tLoss: 29.255173\n",
            "Train Epoch: 72 [12800/60000 (21%)]\tRecon: 28.645748\n",
            "Train Epoch: 72 [12800/60000 (21%)]\tKL: 0.609426\n",
            "\n",
            "Train Epoch: 72 [25600/60000 (43%)]\tLoss: 29.045393\n",
            "Train Epoch: 72 [25600/60000 (43%)]\tRecon: 28.454557\n",
            "Train Epoch: 72 [25600/60000 (43%)]\tKL: 0.590835\n",
            "\n",
            "Train Epoch: 72 [38400/60000 (64%)]\tLoss: 30.374975\n",
            "Train Epoch: 72 [38400/60000 (64%)]\tRecon: 29.398026\n",
            "Train Epoch: 72 [38400/60000 (64%)]\tKL: 0.976950\n",
            "\n",
            "Train Epoch: 72 [51200/60000 (85%)]\tLoss: 29.601994\n",
            "Train Epoch: 72 [51200/60000 (85%)]\tRecon: 28.726032\n",
            "Train Epoch: 72 [51200/60000 (85%)]\tKL: 0.875961\n",
            "\n",
            "====> Epoch: 72 Average loss: 29.8851\n",
            "====> Test set loss: 179.0956\n",
            "Train Epoch: 73 [0/60000 (0%)]\tLoss: 30.453033\n",
            "Train Epoch: 73 [0/60000 (0%)]\tRecon: 29.793221\n",
            "Train Epoch: 73 [0/60000 (0%)]\tKL: 0.659814\n",
            "\n",
            "Train Epoch: 73 [12800/60000 (21%)]\tLoss: 31.051563\n",
            "Train Epoch: 73 [12800/60000 (21%)]\tRecon: 30.357981\n",
            "Train Epoch: 73 [12800/60000 (21%)]\tKL: 0.693582\n",
            "\n",
            "Train Epoch: 73 [25600/60000 (43%)]\tLoss: 30.005024\n",
            "Train Epoch: 73 [25600/60000 (43%)]\tRecon: 29.344078\n",
            "Train Epoch: 73 [25600/60000 (43%)]\tKL: 0.660947\n",
            "\n",
            "Train Epoch: 73 [38400/60000 (64%)]\tLoss: 29.173738\n",
            "Train Epoch: 73 [38400/60000 (64%)]\tRecon: 28.534962\n",
            "Train Epoch: 73 [38400/60000 (64%)]\tKL: 0.638777\n",
            "\n",
            "Train Epoch: 73 [51200/60000 (85%)]\tLoss: 30.383484\n",
            "Train Epoch: 73 [51200/60000 (85%)]\tRecon: 29.546816\n",
            "Train Epoch: 73 [51200/60000 (85%)]\tKL: 0.836668\n",
            "\n",
            "====> Epoch: 73 Average loss: 29.9512\n",
            "====> Test set loss: 178.5658\n",
            "Train Epoch: 74 [0/60000 (0%)]\tLoss: 29.285484\n",
            "Train Epoch: 74 [0/60000 (0%)]\tRecon: 28.902277\n",
            "Train Epoch: 74 [0/60000 (0%)]\tKL: 0.383208\n",
            "\n",
            "Train Epoch: 74 [12800/60000 (21%)]\tLoss: 30.024010\n",
            "Train Epoch: 74 [12800/60000 (21%)]\tRecon: 29.390858\n",
            "Train Epoch: 74 [12800/60000 (21%)]\tKL: 0.633152\n",
            "\n",
            "Train Epoch: 74 [25600/60000 (43%)]\tLoss: 29.744005\n",
            "Train Epoch: 74 [25600/60000 (43%)]\tRecon: 29.226311\n",
            "Train Epoch: 74 [25600/60000 (43%)]\tKL: 0.517694\n",
            "\n",
            "Train Epoch: 74 [38400/60000 (64%)]\tLoss: 30.946018\n",
            "Train Epoch: 74 [38400/60000 (64%)]\tRecon: 30.329521\n",
            "Train Epoch: 74 [38400/60000 (64%)]\tKL: 0.616497\n",
            "\n",
            "Train Epoch: 74 [51200/60000 (85%)]\tLoss: 30.214308\n",
            "Train Epoch: 74 [51200/60000 (85%)]\tRecon: 29.211874\n",
            "Train Epoch: 74 [51200/60000 (85%)]\tKL: 1.002433\n",
            "\n",
            "====> Epoch: 74 Average loss: 30.0842\n",
            "====> Test set loss: 184.2715\n",
            "Train Epoch: 75 [0/60000 (0%)]\tLoss: 31.064274\n",
            "Train Epoch: 75 [0/60000 (0%)]\tRecon: 29.615253\n",
            "Train Epoch: 75 [0/60000 (0%)]\tKL: 1.449020\n",
            "\n",
            "Train Epoch: 75 [12800/60000 (21%)]\tLoss: 30.653500\n",
            "Train Epoch: 75 [12800/60000 (21%)]\tRecon: 29.687567\n",
            "Train Epoch: 75 [12800/60000 (21%)]\tKL: 0.965933\n",
            "\n",
            "Train Epoch: 75 [25600/60000 (43%)]\tLoss: 29.961203\n",
            "Train Epoch: 75 [25600/60000 (43%)]\tRecon: 28.936329\n",
            "Train Epoch: 75 [25600/60000 (43%)]\tKL: 1.024873\n",
            "\n",
            "Train Epoch: 75 [38400/60000 (64%)]\tLoss: 31.121510\n",
            "Train Epoch: 75 [38400/60000 (64%)]\tRecon: 30.186384\n",
            "Train Epoch: 75 [38400/60000 (64%)]\tKL: 0.935124\n",
            "\n",
            "Train Epoch: 75 [51200/60000 (85%)]\tLoss: 30.058487\n",
            "Train Epoch: 75 [51200/60000 (85%)]\tRecon: 29.283936\n",
            "Train Epoch: 75 [51200/60000 (85%)]\tKL: 0.774552\n",
            "\n",
            "====> Epoch: 75 Average loss: 30.2177\n",
            "====> Test set loss: 179.3389\n",
            "Train Epoch: 76 [0/60000 (0%)]\tLoss: 29.705187\n",
            "Train Epoch: 76 [0/60000 (0%)]\tRecon: 29.140953\n",
            "Train Epoch: 76 [0/60000 (0%)]\tKL: 0.564234\n",
            "\n",
            "Train Epoch: 76 [12800/60000 (21%)]\tLoss: 30.050308\n",
            "Train Epoch: 76 [12800/60000 (21%)]\tRecon: 29.429108\n",
            "Train Epoch: 76 [12800/60000 (21%)]\tKL: 0.621200\n",
            "\n",
            "Train Epoch: 76 [25600/60000 (43%)]\tLoss: 30.467503\n",
            "Train Epoch: 76 [25600/60000 (43%)]\tRecon: 29.902987\n",
            "Train Epoch: 76 [25600/60000 (43%)]\tKL: 0.564515\n",
            "\n",
            "Train Epoch: 76 [38400/60000 (64%)]\tLoss: 30.934649\n",
            "Train Epoch: 76 [38400/60000 (64%)]\tRecon: 30.208626\n",
            "Train Epoch: 76 [38400/60000 (64%)]\tKL: 0.726023\n",
            "\n",
            "Train Epoch: 76 [51200/60000 (85%)]\tLoss: 31.452785\n",
            "Train Epoch: 76 [51200/60000 (85%)]\tRecon: 29.605751\n",
            "Train Epoch: 76 [51200/60000 (85%)]\tKL: 1.847035\n",
            "\n",
            "====> Epoch: 76 Average loss: 30.8761\n",
            "====> Test set loss: 193.0169\n",
            "Train Epoch: 77 [0/60000 (0%)]\tLoss: 31.493820\n",
            "Train Epoch: 77 [0/60000 (0%)]\tRecon: 29.645714\n",
            "Train Epoch: 77 [0/60000 (0%)]\tKL: 1.848106\n",
            "\n",
            "Train Epoch: 77 [12800/60000 (21%)]\tLoss: 32.015820\n",
            "Train Epoch: 77 [12800/60000 (21%)]\tRecon: 29.926807\n",
            "Train Epoch: 77 [12800/60000 (21%)]\tKL: 2.089012\n",
            "\n",
            "Train Epoch: 77 [25600/60000 (43%)]\tLoss: 30.405159\n",
            "Train Epoch: 77 [25600/60000 (43%)]\tRecon: 29.144964\n",
            "Train Epoch: 77 [25600/60000 (43%)]\tKL: 1.260194\n",
            "\n",
            "Train Epoch: 77 [38400/60000 (64%)]\tLoss: 31.025421\n",
            "Train Epoch: 77 [38400/60000 (64%)]\tRecon: 30.011959\n",
            "Train Epoch: 77 [38400/60000 (64%)]\tKL: 1.013462\n",
            "\n",
            "Train Epoch: 77 [51200/60000 (85%)]\tLoss: 30.962481\n",
            "Train Epoch: 77 [51200/60000 (85%)]\tRecon: 30.025719\n",
            "Train Epoch: 77 [51200/60000 (85%)]\tKL: 0.936762\n",
            "\n",
            "====> Epoch: 77 Average loss: 30.7878\n",
            "====> Test set loss: 181.7644\n",
            "Train Epoch: 78 [0/60000 (0%)]\tLoss: 30.205688\n",
            "Train Epoch: 78 [0/60000 (0%)]\tRecon: 29.415426\n",
            "Train Epoch: 78 [0/60000 (0%)]\tKL: 0.790263\n",
            "\n",
            "Train Epoch: 78 [12800/60000 (21%)]\tLoss: 30.344213\n",
            "Train Epoch: 78 [12800/60000 (21%)]\tRecon: 29.604633\n",
            "Train Epoch: 78 [12800/60000 (21%)]\tKL: 0.739580\n",
            "\n",
            "Train Epoch: 78 [25600/60000 (43%)]\tLoss: 29.531878\n",
            "Train Epoch: 78 [25600/60000 (43%)]\tRecon: 28.925644\n",
            "Train Epoch: 78 [25600/60000 (43%)]\tKL: 0.606233\n",
            "\n",
            "Train Epoch: 78 [38400/60000 (64%)]\tLoss: 30.354126\n",
            "Train Epoch: 78 [38400/60000 (64%)]\tRecon: 29.677437\n",
            "Train Epoch: 78 [38400/60000 (64%)]\tKL: 0.676690\n",
            "\n",
            "Train Epoch: 78 [51200/60000 (85%)]\tLoss: 29.607525\n",
            "Train Epoch: 78 [51200/60000 (85%)]\tRecon: 28.906452\n",
            "Train Epoch: 78 [51200/60000 (85%)]\tKL: 0.701073\n",
            "\n",
            "====> Epoch: 78 Average loss: 30.4898\n",
            "====> Test set loss: 181.8408\n",
            "Train Epoch: 79 [0/60000 (0%)]\tLoss: 30.299433\n",
            "Train Epoch: 79 [0/60000 (0%)]\tRecon: 29.621122\n",
            "Train Epoch: 79 [0/60000 (0%)]\tKL: 0.678310\n",
            "\n",
            "Train Epoch: 79 [12800/60000 (21%)]\tLoss: 30.201128\n",
            "Train Epoch: 79 [12800/60000 (21%)]\tRecon: 29.629589\n",
            "Train Epoch: 79 [12800/60000 (21%)]\tKL: 0.571540\n",
            "\n",
            "Train Epoch: 79 [25600/60000 (43%)]\tLoss: 30.533747\n",
            "Train Epoch: 79 [25600/60000 (43%)]\tRecon: 29.850189\n",
            "Train Epoch: 79 [25600/60000 (43%)]\tKL: 0.683557\n",
            "\n",
            "Train Epoch: 79 [38400/60000 (64%)]\tLoss: 29.341667\n",
            "Train Epoch: 79 [38400/60000 (64%)]\tRecon: 28.873928\n",
            "Train Epoch: 79 [38400/60000 (64%)]\tKL: 0.467740\n",
            "\n",
            "Train Epoch: 79 [51200/60000 (85%)]\tLoss: 30.908447\n",
            "Train Epoch: 79 [51200/60000 (85%)]\tRecon: 29.853086\n",
            "Train Epoch: 79 [51200/60000 (85%)]\tKL: 1.055361\n",
            "\n",
            "====> Epoch: 79 Average loss: 30.3084\n",
            "====> Test set loss: 181.5638\n",
            "Train Epoch: 80 [0/60000 (0%)]\tLoss: 30.624847\n",
            "Train Epoch: 80 [0/60000 (0%)]\tRecon: 29.903193\n",
            "Train Epoch: 80 [0/60000 (0%)]\tKL: 0.721655\n",
            "\n",
            "Train Epoch: 80 [12800/60000 (21%)]\tLoss: 29.939905\n",
            "Train Epoch: 80 [12800/60000 (21%)]\tRecon: 29.303125\n",
            "Train Epoch: 80 [12800/60000 (21%)]\tKL: 0.636779\n",
            "\n",
            "Train Epoch: 80 [25600/60000 (43%)]\tLoss: 29.515369\n",
            "Train Epoch: 80 [25600/60000 (43%)]\tRecon: 28.863911\n",
            "Train Epoch: 80 [25600/60000 (43%)]\tKL: 0.651459\n",
            "\n",
            "Train Epoch: 80 [38400/60000 (64%)]\tLoss: 29.160824\n",
            "Train Epoch: 80 [38400/60000 (64%)]\tRecon: 28.716621\n",
            "Train Epoch: 80 [38400/60000 (64%)]\tKL: 0.444203\n",
            "\n",
            "Train Epoch: 80 [51200/60000 (85%)]\tLoss: 29.678995\n",
            "Train Epoch: 80 [51200/60000 (85%)]\tRecon: 29.241726\n",
            "Train Epoch: 80 [51200/60000 (85%)]\tKL: 0.437269\n",
            "\n",
            "====> Epoch: 80 Average loss: 30.3377\n",
            "====> Test set loss: 181.3289\n",
            "Train Epoch: 81 [0/60000 (0%)]\tLoss: 31.222523\n",
            "Train Epoch: 81 [0/60000 (0%)]\tRecon: 30.564581\n",
            "Train Epoch: 81 [0/60000 (0%)]\tKL: 0.657942\n",
            "\n",
            "Train Epoch: 81 [12800/60000 (21%)]\tLoss: 30.649385\n",
            "Train Epoch: 81 [12800/60000 (21%)]\tRecon: 29.605936\n",
            "Train Epoch: 81 [12800/60000 (21%)]\tKL: 1.043449\n",
            "\n",
            "Train Epoch: 81 [25600/60000 (43%)]\tLoss: 30.118155\n",
            "Train Epoch: 81 [25600/60000 (43%)]\tRecon: 29.570026\n",
            "Train Epoch: 81 [25600/60000 (43%)]\tKL: 0.548128\n",
            "\n",
            "Train Epoch: 81 [38400/60000 (64%)]\tLoss: 30.042238\n",
            "Train Epoch: 81 [38400/60000 (64%)]\tRecon: 29.366875\n",
            "Train Epoch: 81 [38400/60000 (64%)]\tKL: 0.675364\n",
            "\n",
            "Train Epoch: 81 [51200/60000 (85%)]\tLoss: 29.442097\n",
            "Train Epoch: 81 [51200/60000 (85%)]\tRecon: 28.677269\n",
            "Train Epoch: 81 [51200/60000 (85%)]\tKL: 0.764827\n",
            "\n",
            "====> Epoch: 81 Average loss: 30.2623\n",
            "====> Test set loss: 179.6308\n",
            "Train Epoch: 82 [0/60000 (0%)]\tLoss: 29.665825\n",
            "Train Epoch: 82 [0/60000 (0%)]\tRecon: 29.215721\n",
            "Train Epoch: 82 [0/60000 (0%)]\tKL: 0.450104\n",
            "\n",
            "Train Epoch: 82 [12800/60000 (21%)]\tLoss: 30.470091\n",
            "Train Epoch: 82 [12800/60000 (21%)]\tRecon: 29.976542\n",
            "Train Epoch: 82 [12800/60000 (21%)]\tKL: 0.493549\n",
            "\n",
            "Train Epoch: 82 [25600/60000 (43%)]\tLoss: 30.250862\n",
            "Train Epoch: 82 [25600/60000 (43%)]\tRecon: 29.747902\n",
            "Train Epoch: 82 [25600/60000 (43%)]\tKL: 0.502960\n",
            "\n",
            "Train Epoch: 82 [38400/60000 (64%)]\tLoss: 31.632162\n",
            "Train Epoch: 82 [38400/60000 (64%)]\tRecon: 30.160261\n",
            "Train Epoch: 82 [38400/60000 (64%)]\tKL: 1.471901\n",
            "\n",
            "Train Epoch: 82 [51200/60000 (85%)]\tLoss: 31.253004\n",
            "Train Epoch: 82 [51200/60000 (85%)]\tRecon: 30.190487\n",
            "Train Epoch: 82 [51200/60000 (85%)]\tKL: 1.062518\n",
            "\n",
            "====> Epoch: 82 Average loss: 30.6278\n",
            "====> Test set loss: 185.6011\n",
            "Train Epoch: 83 [0/60000 (0%)]\tLoss: 31.566059\n",
            "Train Epoch: 83 [0/60000 (0%)]\tRecon: 30.498022\n",
            "Train Epoch: 83 [0/60000 (0%)]\tKL: 1.068036\n",
            "\n",
            "Train Epoch: 83 [12800/60000 (21%)]\tLoss: 30.626101\n",
            "Train Epoch: 83 [12800/60000 (21%)]\tRecon: 28.824926\n",
            "Train Epoch: 83 [12800/60000 (21%)]\tKL: 1.801174\n",
            "\n",
            "Train Epoch: 83 [25600/60000 (43%)]\tLoss: 30.361237\n",
            "Train Epoch: 83 [25600/60000 (43%)]\tRecon: 29.465794\n",
            "Train Epoch: 83 [25600/60000 (43%)]\tKL: 0.895444\n",
            "\n",
            "Train Epoch: 83 [38400/60000 (64%)]\tLoss: 30.630646\n",
            "Train Epoch: 83 [38400/60000 (64%)]\tRecon: 29.412668\n",
            "Train Epoch: 83 [38400/60000 (64%)]\tKL: 1.217978\n",
            "\n",
            "Train Epoch: 83 [51200/60000 (85%)]\tLoss: 30.544737\n",
            "Train Epoch: 83 [51200/60000 (85%)]\tRecon: 29.625355\n",
            "Train Epoch: 83 [51200/60000 (85%)]\tKL: 0.919381\n",
            "\n",
            "====> Epoch: 83 Average loss: 30.7515\n",
            "====> Test set loss: 181.1617\n",
            "Train Epoch: 84 [0/60000 (0%)]\tLoss: 30.402296\n",
            "Train Epoch: 84 [0/60000 (0%)]\tRecon: 29.727093\n",
            "Train Epoch: 84 [0/60000 (0%)]\tKL: 0.675204\n",
            "\n",
            "Train Epoch: 84 [12800/60000 (21%)]\tLoss: 31.279322\n",
            "Train Epoch: 84 [12800/60000 (21%)]\tRecon: 29.627789\n",
            "Train Epoch: 84 [12800/60000 (21%)]\tKL: 1.651534\n",
            "\n",
            "Train Epoch: 84 [25600/60000 (43%)]\tLoss: 31.773155\n",
            "Train Epoch: 84 [25600/60000 (43%)]\tRecon: 31.003830\n",
            "Train Epoch: 84 [25600/60000 (43%)]\tKL: 0.769326\n",
            "\n",
            "Train Epoch: 84 [38400/60000 (64%)]\tLoss: 31.510284\n",
            "Train Epoch: 84 [38400/60000 (64%)]\tRecon: 30.809982\n",
            "Train Epoch: 84 [38400/60000 (64%)]\tKL: 0.700302\n",
            "\n",
            "Train Epoch: 84 [51200/60000 (85%)]\tLoss: 30.143675\n",
            "Train Epoch: 84 [51200/60000 (85%)]\tRecon: 29.253479\n",
            "Train Epoch: 84 [51200/60000 (85%)]\tKL: 0.890196\n",
            "\n",
            "====> Epoch: 84 Average loss: 30.4863\n",
            "====> Test set loss: 185.5736\n",
            "Train Epoch: 85 [0/60000 (0%)]\tLoss: 30.764858\n",
            "Train Epoch: 85 [0/60000 (0%)]\tRecon: 29.727898\n",
            "Train Epoch: 85 [0/60000 (0%)]\tKL: 1.036961\n",
            "\n",
            "Train Epoch: 85 [12800/60000 (21%)]\tLoss: 30.945564\n",
            "Train Epoch: 85 [12800/60000 (21%)]\tRecon: 30.306606\n",
            "Train Epoch: 85 [12800/60000 (21%)]\tKL: 0.638959\n",
            "\n",
            "Train Epoch: 85 [25600/60000 (43%)]\tLoss: 29.574945\n",
            "Train Epoch: 85 [25600/60000 (43%)]\tRecon: 29.076262\n",
            "Train Epoch: 85 [25600/60000 (43%)]\tKL: 0.498685\n",
            "\n",
            "Train Epoch: 85 [38400/60000 (64%)]\tLoss: 30.617125\n",
            "Train Epoch: 85 [38400/60000 (64%)]\tRecon: 30.003597\n",
            "Train Epoch: 85 [38400/60000 (64%)]\tKL: 0.613528\n",
            "\n",
            "Train Epoch: 85 [51200/60000 (85%)]\tLoss: 29.463598\n",
            "Train Epoch: 85 [51200/60000 (85%)]\tRecon: 28.982487\n",
            "Train Epoch: 85 [51200/60000 (85%)]\tKL: 0.481112\n",
            "\n",
            "====> Epoch: 85 Average loss: 30.3211\n",
            "====> Test set loss: 180.8789\n",
            "Train Epoch: 86 [0/60000 (0%)]\tLoss: 30.054522\n",
            "Train Epoch: 86 [0/60000 (0%)]\tRecon: 29.354820\n",
            "Train Epoch: 86 [0/60000 (0%)]\tKL: 0.699701\n",
            "\n",
            "Train Epoch: 86 [12800/60000 (21%)]\tLoss: 30.379904\n",
            "Train Epoch: 86 [12800/60000 (21%)]\tRecon: 29.872828\n",
            "Train Epoch: 86 [12800/60000 (21%)]\tKL: 0.507076\n",
            "\n",
            "Train Epoch: 86 [25600/60000 (43%)]\tLoss: 30.990982\n",
            "Train Epoch: 86 [25600/60000 (43%)]\tRecon: 30.375282\n",
            "Train Epoch: 86 [25600/60000 (43%)]\tKL: 0.615700\n",
            "\n",
            "Train Epoch: 86 [38400/60000 (64%)]\tLoss: 30.738525\n",
            "Train Epoch: 86 [38400/60000 (64%)]\tRecon: 30.321640\n",
            "Train Epoch: 86 [38400/60000 (64%)]\tKL: 0.416885\n",
            "\n",
            "Train Epoch: 86 [51200/60000 (85%)]\tLoss: 29.810856\n",
            "Train Epoch: 86 [51200/60000 (85%)]\tRecon: 29.410109\n",
            "Train Epoch: 86 [51200/60000 (85%)]\tKL: 0.400748\n",
            "\n",
            "====> Epoch: 86 Average loss: 30.2513\n",
            "====> Test set loss: 183.7449\n",
            "Train Epoch: 87 [0/60000 (0%)]\tLoss: 29.960453\n",
            "Train Epoch: 87 [0/60000 (0%)]\tRecon: 29.230179\n",
            "Train Epoch: 87 [0/60000 (0%)]\tKL: 0.730274\n",
            "\n",
            "Train Epoch: 87 [12800/60000 (21%)]\tLoss: 29.483791\n",
            "Train Epoch: 87 [12800/60000 (21%)]\tRecon: 29.050098\n",
            "Train Epoch: 87 [12800/60000 (21%)]\tKL: 0.433692\n",
            "\n",
            "Train Epoch: 87 [25600/60000 (43%)]\tLoss: 30.154219\n",
            "Train Epoch: 87 [25600/60000 (43%)]\tRecon: 29.766020\n",
            "Train Epoch: 87 [25600/60000 (43%)]\tKL: 0.388198\n",
            "\n",
            "Train Epoch: 87 [38400/60000 (64%)]\tLoss: 30.006903\n",
            "Train Epoch: 87 [38400/60000 (64%)]\tRecon: 29.265873\n",
            "Train Epoch: 87 [38400/60000 (64%)]\tKL: 0.741030\n",
            "\n",
            "Train Epoch: 87 [51200/60000 (85%)]\tLoss: 31.438931\n",
            "Train Epoch: 87 [51200/60000 (85%)]\tRecon: 29.337776\n",
            "Train Epoch: 87 [51200/60000 (85%)]\tKL: 2.101155\n",
            "\n",
            "====> Epoch: 87 Average loss: 30.3494\n",
            "====> Test set loss: 182.7685\n",
            "Train Epoch: 88 [0/60000 (0%)]\tLoss: 30.570543\n",
            "Train Epoch: 88 [0/60000 (0%)]\tRecon: 29.435204\n",
            "Train Epoch: 88 [0/60000 (0%)]\tKL: 1.135340\n",
            "\n",
            "Train Epoch: 88 [12800/60000 (21%)]\tLoss: 30.320787\n",
            "Train Epoch: 88 [12800/60000 (21%)]\tRecon: 29.519114\n",
            "Train Epoch: 88 [12800/60000 (21%)]\tKL: 0.801674\n",
            "\n",
            "Train Epoch: 88 [25600/60000 (43%)]\tLoss: 29.311039\n",
            "Train Epoch: 88 [25600/60000 (43%)]\tRecon: 28.966820\n",
            "Train Epoch: 88 [25600/60000 (43%)]\tKL: 0.344219\n",
            "\n",
            "Train Epoch: 88 [38400/60000 (64%)]\tLoss: 29.848103\n",
            "Train Epoch: 88 [38400/60000 (64%)]\tRecon: 29.230099\n",
            "Train Epoch: 88 [38400/60000 (64%)]\tKL: 0.618003\n",
            "\n",
            "Train Epoch: 88 [51200/60000 (85%)]\tLoss: 31.548973\n",
            "Train Epoch: 88 [51200/60000 (85%)]\tRecon: 29.938276\n",
            "Train Epoch: 88 [51200/60000 (85%)]\tKL: 1.610697\n",
            "\n",
            "====> Epoch: 88 Average loss: 30.6006\n",
            "====> Test set loss: 186.4398\n",
            "Train Epoch: 89 [0/60000 (0%)]\tLoss: 31.063971\n",
            "Train Epoch: 89 [0/60000 (0%)]\tRecon: 29.897350\n",
            "Train Epoch: 89 [0/60000 (0%)]\tKL: 1.166620\n",
            "\n",
            "Train Epoch: 89 [12800/60000 (21%)]\tLoss: 29.769201\n",
            "Train Epoch: 89 [12800/60000 (21%)]\tRecon: 29.168522\n",
            "Train Epoch: 89 [12800/60000 (21%)]\tKL: 0.600679\n",
            "\n",
            "Train Epoch: 89 [25600/60000 (43%)]\tLoss: 30.393656\n",
            "Train Epoch: 89 [25600/60000 (43%)]\tRecon: 29.299463\n",
            "Train Epoch: 89 [25600/60000 (43%)]\tKL: 1.094193\n",
            "\n",
            "Train Epoch: 89 [38400/60000 (64%)]\tLoss: 30.242102\n",
            "Train Epoch: 89 [38400/60000 (64%)]\tRecon: 28.821877\n",
            "Train Epoch: 89 [38400/60000 (64%)]\tKL: 1.420226\n",
            "\n",
            "Train Epoch: 89 [51200/60000 (85%)]\tLoss: 34.330334\n",
            "Train Epoch: 89 [51200/60000 (85%)]\tRecon: 29.253790\n",
            "Train Epoch: 89 [51200/60000 (85%)]\tKL: 5.076542\n",
            "\n",
            "====> Epoch: 89 Average loss: 34.2934\n",
            "====> Test set loss: 202.6560\n",
            "Train Epoch: 90 [0/60000 (0%)]\tLoss: 32.659325\n",
            "Train Epoch: 90 [0/60000 (0%)]\tRecon: 28.518999\n",
            "Train Epoch: 90 [0/60000 (0%)]\tKL: 4.140324\n",
            "\n",
            "Train Epoch: 90 [12800/60000 (21%)]\tLoss: 50.093861\n",
            "Train Epoch: 90 [12800/60000 (21%)]\tRecon: 29.727562\n",
            "Train Epoch: 90 [12800/60000 (21%)]\tKL: 20.366299\n",
            "\n",
            "Train Epoch: 90 [25600/60000 (43%)]\tLoss: 32.760288\n",
            "Train Epoch: 90 [25600/60000 (43%)]\tRecon: 29.617966\n",
            "Train Epoch: 90 [25600/60000 (43%)]\tKL: 3.142321\n",
            "\n",
            "Train Epoch: 90 [38400/60000 (64%)]\tLoss: 52.676384\n",
            "Train Epoch: 90 [38400/60000 (64%)]\tRecon: 29.127966\n",
            "Train Epoch: 90 [38400/60000 (64%)]\tKL: 23.548418\n",
            "\n",
            "Train Epoch: 90 [51200/60000 (85%)]\tLoss: 31.683830\n",
            "Train Epoch: 90 [51200/60000 (85%)]\tRecon: 29.969030\n",
            "Train Epoch: 90 [51200/60000 (85%)]\tKL: 1.714799\n",
            "\n",
            "====> Epoch: 90 Average loss: 36.0727\n",
            "====> Test set loss: 239.1684\n",
            "Train Epoch: 91 [0/60000 (0%)]\tLoss: 38.934639\n",
            "Train Epoch: 91 [0/60000 (0%)]\tRecon: 29.384188\n",
            "Train Epoch: 91 [0/60000 (0%)]\tKL: 9.550453\n",
            "\n",
            "Train Epoch: 91 [12800/60000 (21%)]\tLoss: 31.213629\n",
            "Train Epoch: 91 [12800/60000 (21%)]\tRecon: 29.542435\n",
            "Train Epoch: 91 [12800/60000 (21%)]\tKL: 1.671194\n",
            "\n",
            "Train Epoch: 91 [25600/60000 (43%)]\tLoss: 30.974176\n",
            "Train Epoch: 91 [25600/60000 (43%)]\tRecon: 29.480206\n",
            "Train Epoch: 91 [25600/60000 (43%)]\tKL: 1.493971\n",
            "\n",
            "Train Epoch: 91 [38400/60000 (64%)]\tLoss: 30.185968\n",
            "Train Epoch: 91 [38400/60000 (64%)]\tRecon: 29.019491\n",
            "Train Epoch: 91 [38400/60000 (64%)]\tKL: 1.166478\n",
            "\n",
            "Train Epoch: 91 [51200/60000 (85%)]\tLoss: 30.915913\n",
            "Train Epoch: 91 [51200/60000 (85%)]\tRecon: 29.919445\n",
            "Train Epoch: 91 [51200/60000 (85%)]\tKL: 0.996468\n",
            "\n",
            "====> Epoch: 91 Average loss: 31.3195\n",
            "====> Test set loss: 182.2455\n",
            "Train Epoch: 92 [0/60000 (0%)]\tLoss: 30.313595\n",
            "Train Epoch: 92 [0/60000 (0%)]\tRecon: 29.373861\n",
            "Train Epoch: 92 [0/60000 (0%)]\tKL: 0.939733\n",
            "\n",
            "Train Epoch: 92 [12800/60000 (21%)]\tLoss: 30.522749\n",
            "Train Epoch: 92 [12800/60000 (21%)]\tRecon: 29.741940\n",
            "Train Epoch: 92 [12800/60000 (21%)]\tKL: 0.780809\n",
            "\n",
            "Train Epoch: 92 [25600/60000 (43%)]\tLoss: 32.093903\n",
            "Train Epoch: 92 [25600/60000 (43%)]\tRecon: 30.290668\n",
            "Train Epoch: 92 [25600/60000 (43%)]\tKL: 1.803233\n",
            "\n",
            "Train Epoch: 92 [38400/60000 (64%)]\tLoss: 30.750816\n",
            "Train Epoch: 92 [38400/60000 (64%)]\tRecon: 29.149572\n",
            "Train Epoch: 92 [38400/60000 (64%)]\tKL: 1.601244\n",
            "\n",
            "Train Epoch: 92 [51200/60000 (85%)]\tLoss: 34.141994\n",
            "Train Epoch: 92 [51200/60000 (85%)]\tRecon: 29.363653\n",
            "Train Epoch: 92 [51200/60000 (85%)]\tKL: 4.778341\n",
            "\n",
            "====> Epoch: 92 Average loss: 32.6442\n",
            "====> Test set loss: 187.6537\n",
            "Train Epoch: 93 [0/60000 (0%)]\tLoss: 31.027517\n",
            "Train Epoch: 93 [0/60000 (0%)]\tRecon: 29.343655\n",
            "Train Epoch: 93 [0/60000 (0%)]\tKL: 1.683863\n",
            "\n",
            "Train Epoch: 93 [12800/60000 (21%)]\tLoss: 31.154013\n",
            "Train Epoch: 93 [12800/60000 (21%)]\tRecon: 29.741859\n",
            "Train Epoch: 93 [12800/60000 (21%)]\tKL: 1.412154\n",
            "\n",
            "Train Epoch: 93 [25600/60000 (43%)]\tLoss: 30.497450\n",
            "Train Epoch: 93 [25600/60000 (43%)]\tRecon: 29.605162\n",
            "Train Epoch: 93 [25600/60000 (43%)]\tKL: 0.892288\n",
            "\n",
            "Train Epoch: 93 [38400/60000 (64%)]\tLoss: 30.943954\n",
            "Train Epoch: 93 [38400/60000 (64%)]\tRecon: 29.966785\n",
            "Train Epoch: 93 [38400/60000 (64%)]\tKL: 0.977169\n",
            "\n",
            "Train Epoch: 93 [51200/60000 (85%)]\tLoss: 31.235849\n",
            "Train Epoch: 93 [51200/60000 (85%)]\tRecon: 30.184135\n",
            "Train Epoch: 93 [51200/60000 (85%)]\tKL: 1.051714\n",
            "\n",
            "====> Epoch: 93 Average loss: 31.2146\n",
            "====> Test set loss: 182.7080\n",
            "Train Epoch: 94 [0/60000 (0%)]\tLoss: 30.111750\n",
            "Train Epoch: 94 [0/60000 (0%)]\tRecon: 29.318451\n",
            "Train Epoch: 94 [0/60000 (0%)]\tKL: 0.793299\n",
            "\n",
            "Train Epoch: 94 [12800/60000 (21%)]\tLoss: 47.799683\n",
            "Train Epoch: 94 [12800/60000 (21%)]\tRecon: 28.878965\n",
            "Train Epoch: 94 [12800/60000 (21%)]\tKL: 18.920717\n",
            "\n",
            "Train Epoch: 94 [25600/60000 (43%)]\tLoss: 34.557091\n",
            "Train Epoch: 94 [25600/60000 (43%)]\tRecon: 29.908264\n",
            "Train Epoch: 94 [25600/60000 (43%)]\tKL: 4.648826\n",
            "\n",
            "Train Epoch: 94 [38400/60000 (64%)]\tLoss: 31.702572\n",
            "Train Epoch: 94 [38400/60000 (64%)]\tRecon: 29.621941\n",
            "Train Epoch: 94 [38400/60000 (64%)]\tKL: 2.080632\n",
            "\n",
            "Train Epoch: 94 [51200/60000 (85%)]\tLoss: 31.376076\n",
            "Train Epoch: 94 [51200/60000 (85%)]\tRecon: 29.893475\n",
            "Train Epoch: 94 [51200/60000 (85%)]\tKL: 1.482601\n",
            "\n",
            "====> Epoch: 94 Average loss: 36.1463\n",
            "====> Test set loss: 185.7447\n",
            "Train Epoch: 95 [0/60000 (0%)]\tLoss: 31.657139\n",
            "Train Epoch: 95 [0/60000 (0%)]\tRecon: 30.341751\n",
            "Train Epoch: 95 [0/60000 (0%)]\tKL: 1.315388\n",
            "\n",
            "Train Epoch: 95 [12800/60000 (21%)]\tLoss: 30.733534\n",
            "Train Epoch: 95 [12800/60000 (21%)]\tRecon: 29.642370\n",
            "Train Epoch: 95 [12800/60000 (21%)]\tKL: 1.091163\n",
            "\n",
            "Train Epoch: 95 [25600/60000 (43%)]\tLoss: 30.892658\n",
            "Train Epoch: 95 [25600/60000 (43%)]\tRecon: 29.638647\n",
            "Train Epoch: 95 [25600/60000 (43%)]\tKL: 1.254011\n",
            "\n",
            "Train Epoch: 95 [38400/60000 (64%)]\tLoss: 30.641081\n",
            "Train Epoch: 95 [38400/60000 (64%)]\tRecon: 29.235840\n",
            "Train Epoch: 95 [38400/60000 (64%)]\tKL: 1.405241\n",
            "\n",
            "Train Epoch: 95 [51200/60000 (85%)]\tLoss: 30.616486\n",
            "Train Epoch: 95 [51200/60000 (85%)]\tRecon: 29.851391\n",
            "Train Epoch: 95 [51200/60000 (85%)]\tKL: 0.765095\n",
            "\n",
            "====> Epoch: 95 Average loss: 30.6212\n",
            "====> Test set loss: 181.5840\n",
            "Train Epoch: 96 [0/60000 (0%)]\tLoss: 30.481529\n",
            "Train Epoch: 96 [0/60000 (0%)]\tRecon: 29.728096\n",
            "Train Epoch: 96 [0/60000 (0%)]\tKL: 0.753433\n",
            "\n",
            "Train Epoch: 96 [12800/60000 (21%)]\tLoss: 35.267292\n",
            "Train Epoch: 96 [12800/60000 (21%)]\tRecon: 29.392761\n",
            "Train Epoch: 96 [12800/60000 (21%)]\tKL: 5.874531\n",
            "\n",
            "Train Epoch: 96 [25600/60000 (43%)]\tLoss: 31.525156\n",
            "Train Epoch: 96 [25600/60000 (43%)]\tRecon: 30.010159\n",
            "Train Epoch: 96 [25600/60000 (43%)]\tKL: 1.514998\n",
            "\n",
            "Train Epoch: 96 [38400/60000 (64%)]\tLoss: 30.911951\n",
            "Train Epoch: 96 [38400/60000 (64%)]\tRecon: 29.591660\n",
            "Train Epoch: 96 [38400/60000 (64%)]\tKL: 1.320291\n",
            "\n",
            "Train Epoch: 96 [51200/60000 (85%)]\tLoss: 30.873150\n",
            "Train Epoch: 96 [51200/60000 (85%)]\tRecon: 29.534203\n",
            "Train Epoch: 96 [51200/60000 (85%)]\tKL: 1.338947\n",
            "\n",
            "====> Epoch: 96 Average loss: 34.2646\n",
            "====> Test set loss: 184.0318\n",
            "Train Epoch: 97 [0/60000 (0%)]\tLoss: 30.813753\n",
            "Train Epoch: 97 [0/60000 (0%)]\tRecon: 29.631664\n",
            "Train Epoch: 97 [0/60000 (0%)]\tKL: 1.182089\n",
            "\n",
            "Train Epoch: 97 [12800/60000 (21%)]\tLoss: 31.465857\n",
            "Train Epoch: 97 [12800/60000 (21%)]\tRecon: 30.514994\n",
            "Train Epoch: 97 [12800/60000 (21%)]\tKL: 0.950863\n",
            "\n",
            "Train Epoch: 97 [25600/60000 (43%)]\tLoss: 31.612221\n",
            "Train Epoch: 97 [25600/60000 (43%)]\tRecon: 29.232422\n",
            "Train Epoch: 97 [25600/60000 (43%)]\tKL: 2.379800\n",
            "\n",
            "Train Epoch: 97 [38400/60000 (64%)]\tLoss: 30.782038\n",
            "Train Epoch: 97 [38400/60000 (64%)]\tRecon: 29.715061\n",
            "Train Epoch: 97 [38400/60000 (64%)]\tKL: 1.066977\n",
            "\n",
            "Train Epoch: 97 [51200/60000 (85%)]\tLoss: 30.409437\n",
            "Train Epoch: 97 [51200/60000 (85%)]\tRecon: 29.247726\n",
            "Train Epoch: 97 [51200/60000 (85%)]\tKL: 1.161710\n",
            "\n",
            "====> Epoch: 97 Average loss: 30.8089\n",
            "====> Test set loss: 185.4358\n",
            "Train Epoch: 98 [0/60000 (0%)]\tLoss: 31.336481\n",
            "Train Epoch: 98 [0/60000 (0%)]\tRecon: 30.060005\n",
            "Train Epoch: 98 [0/60000 (0%)]\tKL: 1.276477\n",
            "\n",
            "Train Epoch: 98 [12800/60000 (21%)]\tLoss: 30.787405\n",
            "Train Epoch: 98 [12800/60000 (21%)]\tRecon: 29.867989\n",
            "Train Epoch: 98 [12800/60000 (21%)]\tKL: 0.919417\n",
            "\n",
            "Train Epoch: 98 [25600/60000 (43%)]\tLoss: 36.607216\n",
            "Train Epoch: 98 [25600/60000 (43%)]\tRecon: 29.978123\n",
            "Train Epoch: 98 [25600/60000 (43%)]\tKL: 6.629093\n",
            "\n",
            "Train Epoch: 98 [38400/60000 (64%)]\tLoss: 34.558426\n",
            "Train Epoch: 98 [38400/60000 (64%)]\tRecon: 28.529297\n",
            "Train Epoch: 98 [38400/60000 (64%)]\tKL: 6.029130\n",
            "\n",
            "Train Epoch: 98 [51200/60000 (85%)]\tLoss: 30.781830\n",
            "Train Epoch: 98 [51200/60000 (85%)]\tRecon: 29.762295\n",
            "Train Epoch: 98 [51200/60000 (85%)]\tKL: 1.019535\n",
            "\n",
            "====> Epoch: 98 Average loss: 31.5456\n",
            "====> Test set loss: 182.5542\n",
            "Train Epoch: 99 [0/60000 (0%)]\tLoss: 30.911295\n",
            "Train Epoch: 99 [0/60000 (0%)]\tRecon: 30.198227\n",
            "Train Epoch: 99 [0/60000 (0%)]\tKL: 0.713068\n",
            "\n",
            "Train Epoch: 99 [12800/60000 (21%)]\tLoss: 30.319906\n",
            "Train Epoch: 99 [12800/60000 (21%)]\tRecon: 29.486416\n",
            "Train Epoch: 99 [12800/60000 (21%)]\tKL: 0.833490\n",
            "\n",
            "Train Epoch: 99 [25600/60000 (43%)]\tLoss: 30.639931\n",
            "Train Epoch: 99 [25600/60000 (43%)]\tRecon: 29.547499\n",
            "Train Epoch: 99 [25600/60000 (43%)]\tKL: 1.092432\n",
            "\n",
            "Train Epoch: 99 [38400/60000 (64%)]\tLoss: 32.300774\n",
            "Train Epoch: 99 [38400/60000 (64%)]\tRecon: 30.514080\n",
            "Train Epoch: 99 [38400/60000 (64%)]\tKL: 1.786692\n",
            "\n",
            "Train Epoch: 99 [51200/60000 (85%)]\tLoss: 32.736485\n",
            "Train Epoch: 99 [51200/60000 (85%)]\tRecon: 29.591076\n",
            "Train Epoch: 99 [51200/60000 (85%)]\tKL: 3.145409\n",
            "\n",
            "====> Epoch: 99 Average loss: 31.4990\n",
            "====> Test set loss: 189.2247\n",
            "Train Epoch: 100 [0/60000 (0%)]\tLoss: 32.248692\n",
            "Train Epoch: 100 [0/60000 (0%)]\tRecon: 30.393627\n",
            "Train Epoch: 100 [0/60000 (0%)]\tKL: 1.855065\n",
            "\n",
            "Train Epoch: 100 [12800/60000 (21%)]\tLoss: 31.423998\n",
            "Train Epoch: 100 [12800/60000 (21%)]\tRecon: 29.982212\n",
            "Train Epoch: 100 [12800/60000 (21%)]\tKL: 1.441785\n",
            "\n",
            "Train Epoch: 100 [25600/60000 (43%)]\tLoss: 31.667439\n",
            "Train Epoch: 100 [25600/60000 (43%)]\tRecon: 30.228956\n",
            "Train Epoch: 100 [25600/60000 (43%)]\tKL: 1.438482\n",
            "\n",
            "Train Epoch: 100 [38400/60000 (64%)]\tLoss: 31.058374\n",
            "Train Epoch: 100 [38400/60000 (64%)]\tRecon: 29.683895\n",
            "Train Epoch: 100 [38400/60000 (64%)]\tKL: 1.374479\n",
            "\n",
            "Train Epoch: 100 [51200/60000 (85%)]\tLoss: 31.350790\n",
            "Train Epoch: 100 [51200/60000 (85%)]\tRecon: 29.091877\n",
            "Train Epoch: 100 [51200/60000 (85%)]\tKL: 2.258913\n",
            "\n",
            "====> Epoch: 100 Average loss: 31.3337\n",
            "====> Test set loss: 184.7008\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oynCbvw_nmnQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}